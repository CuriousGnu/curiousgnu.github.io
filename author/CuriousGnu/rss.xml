<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>www.curiousgnu.com</title>
   
   <link>http://www.curiousgnu.com/</link>
   <description>Numbers, Graphs, and Apple Strudels</description>
   <language>en-uk</language>
   <managingEditor> CuriousGnu</managingEditor>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>78% of Reddit Threads With 1,000+ Comments Mention Nazis</title>
	  <link>//reddit-godwin</link>
	  <author>CuriousGnu</author>
	  <pubDate>2016-05-04T03:00:00+02:00</pubDate>
	  <guid>//reddit-godwin</guid>
	  <description><![CDATA[
	     <p>Let me start this post by noting that I will not attempt to test <a href="https://en.wikipedia.org/wiki/Godwin%27s_law">Godwin’s Law</a>, which states that:</p>

<pre><code>As an online discussion grows longer, the probability of a comparison involving Nazis or Hitler approaches 1.
</code></pre>

<p>In this post, I’ll only try to find out how many Reddit comments mention <em>Nazis</em> or <em>Hitler</em> and ignore the context in which they are made. The data source for this analysis is the <a href="https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2016_03">Reddit dataset</a> which is publicly available on Google BigQuery. The following graph is based on <strong>4.6 million</strong> comments and shows the share of comments mentioning <em>Nazis</em> or <em>Hitler</em> by subreddit.</p>

<p><a href="/assets/images/rd-godwin/subrd.png"><img src="/assets/images/rd-godwin/subrd.png" alt="Hitler Comments by Subreddits" /></a></p>

<p>Then I excluded history subreddits and looked at the probability that a Reddit thread mentions <em>Nazis</em> or <em>Hitler</em> at least once. Unsuprisigly, the probability of a Nazi refrence increases as the threads get bigger. Nevertheless, I didn’t expect that the probability would be <strong>over 70%</strong> for a thread with more than 1,000 comments.</p>

<p><a href="/assets/images/rd-godwin/pbrd.png"><img src="/assets/images/rd-godwin/pbrd.png" alt="Hitler References" style="max-width:780px" /></a></p>

<p>The next step would be to implement sophisticated text mining techniques to identify comments which use Nazi analogies in a way as described by Godwin. Unfortunately due to time constraints and the complexity of this problem, I was not able to try for this blog post.</p>

	  ]]></description>
	</item>

	<item>
	  <title>How The World Sees Hillary Clinton & Donald Trump</title>
	  <link>//how-the-world-sees-clinton-trump</link>
	  <author>CuriousGnu</author>
	  <pubDate>2016-04-27T19:14:00+02:00</pubDate>
	  <guid>//how-the-world-sees-clinton-trump</guid>
	  <description><![CDATA[
	     <p>For this week’s blog post, I will try to find out how the international news media writes about Hillary Clinton and Donald Trump; the presidential front-runners of both parties. The plan is to check on several international news sources to derive positive and negative things regarding Hillary Clinton or Donald Trump. Doing this will give us an idea of how the news media in other countries, writes and talks about the two candidates.</p>

<p>Fortunately, most of the hard work was already done by the <a href="http://gdeltproject.org/">GDELT Project</a>, which monitors news sites from all around the world and makes its work freely available for everyone. They even automatically determine how positive or negative news articles are using sentiment analysis. Based on the <a href="http://gdeltproject.org/data.html">GDELT dataset</a>, I created a map for each candidate which shows how the average tone of the texts compares to American news (Clinton: -1.15; Trump: -1.40). The results are based on a total of over <strong>550,000 articles</strong> published after July 2015 of which 65.3% mentioned <em>Donald Trump</em> at least twice, and 46.1% mentioned <em>Hillary Clinton</em> at least twice.</p>

<div class="emb-iframe">
  <iframe height="612" width="980" frameborder="0" src="https://curiousgnu.cartodb.com/viz/5497e1dc-0991-11e6-87dd-0e5db1731f59/embed_map" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" oallowfullscreen="" msallowfullscreen=""></iframe>
</div>
<div class="emb-iframe">
  <iframe height="612" width="980" frameborder="0" src="https://curiousgnu.cartodb.com/viz/676de180-0991-11e6-9940-0e674067d321/embed_map" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" oallowfullscreen="" msallowfullscreen=""></iframe>
</div>

<p>Compared to the Republican front-runner Donald Trump, international journalists seem to view Hillary Clinton much more positively. Looking at the maps above, we can see that news articles from countries like <em>Mexico</em>, <em>India</em>, or <em>China</em> are clearly more favorable towards Clinton than Trump. One exception is the Russian media which reports 19% more positively about Trump than its American counterpart. I don’t want to get political, but I think the results for some countries aren’t much of a surprise.</p>

<h4 id="technical-background">Technical Background</h4>

<p>The process of doing this analysis is fairly straightforward and does not require anything except a browser and a Google account. First, I used the <a href="http://gdeltproject.org/">GDELT database</a>, publicly available on <a href="https://bigquery.cloud.google.com/table/gdelt-bq:gdeltv2.gkg">Google BigQuery</a>, to extract the raw data needed to create both maps. I wrote the following SQL query to do this:</p>

<pre class="sql"><code>SELECT	a.country
	,AVG(CASE WHEN a.trump = 1 
		THEN a.tone ELSE NULL END) trump_tone
	,AVG(CASE WHEN a.hillary = 1
		THEN a.tone ELSE NULL END) hillary_tone
FROM (SELECT 
  cc.CountryHumanName country
  ,CASE WHEN 
  	LOWER(gkg.AllNames) LIKE '%donald%trump%donald%trump%'
  	THEN 1 ELSE 0 END trump
  ,CASE WHEN
  	LOWER(gkg.AllNames) LIKE '%hillary%clinton%hillary%clinton%'
  	THEN 1 ELSE 0 END hillary
  ,FIRST(SPLIT(gkg.V2Tone, ',')) tone
FROM [gdelt-bq:gdeltv2.gkg] gkg
INNER JOIN [gdelt-bq:gdeltv2.domainsbycountry_alllangs_april2015] cc
  ON cc.Domain = gkg.SourceCommonName
WHERE (
  	LOWER(gkg.AllNames) LIKE '%donald%trump%donald%trump%'
  	OR LOWER(gkg.AllNames) LIKE '%hillary%clinton%hillary%clinton%'
  ) AND gkg.DATE &gt;= 20150801000000
) a
GROUP BY a.country
HAVING	SUM(a.trump) &gt;= 100
	AND SUM(a.hillary) &gt;= 100
</code></pre>

<p>In the second step, I exported the results of the query as a CSV file and uploaded it to <a href="https://cartodb.com/">CartoDB</a>, a free web service where you can create maps based on location-based data. From there on you can follow their documentation and have your maps ready in no time.</p>

<p>From my experience, <a href="https://cartodb.com/">CartoDB</a> is a great tool if you want to create interactive and highly customizable maps. If you only need a basic set of features, you ought to try out <a href="https://www.google.com/sheets/about/">Google Sheets</a>. <a href="http://www.tableau.com/">Tableau</a> is another good alternative that I frequently use, which is also available in the free version <a href="https://public.tableau.com/s/">Tableau Public</a>. I didn’t use Tableau in this project because CartoDB offers much better embedding options for blogs or websites.</p>

<p>If you have any questions about this blog post, feel free to contact me by <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#099;&#111;&#110;&#116;&#097;&#099;&#116;&#064;&#099;&#117;&#114;&#105;&#111;&#117;&#115;&#103;&#110;&#117;&#046;&#099;&#111;&#109;">email</a> or write me a PM on Reddit.</p>

<p> </p>

<p><small>Photos by <a href="https://www.flickr.com/photos/gageskidmore/">Gage Skidmore</a> is licensed under CC BY-SA 2.0</small></p>

	  ]]></description>
	</item>

	<item>
	  <title>Redditors who commented in /r/X also commented in /r/Y</title>
	  <link>//reddit-comments</link>
	  <author>CuriousGnu</author>
	  <pubDate>2016-04-20T14:14:00+02:00</pubDate>
	  <guid>//reddit-comments</guid>
	  <description><![CDATA[
	     <p>This blog article is about <a href="http://www.reddit.com">reddit.com</a>, a website where people can post links to interesting websites and discuss a wide variety of different topics. According to <a href="http://www.alexa.com/topsites/countries/US">Alexa</a>, a company who analyzes web traffic, Reddit is the ninth most popular site in the United States. Reddit has thousands of different subcategories, called subreddits, which are usually moderated by volunteers. There are subreddits for nearly every topic you can imagine; for example, on <a href="http://www.reddit.com/r/movies">/r/movies</a> people can discuss the latest blockbuster whereas the users over at <a href="http://www.reddit.com/r/sloths">/r/sloths</a> are passionately committed to collecting cute pictures of sloths.</p>

<p>But Reddit can also be fascinating to people who are interested in data research because the user generated data is easily accessible via the official API and through <a href="https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2016_01">Google BigQuery</a> where you can find an SQL database which you can use for little to no cost. For this article, I decided to start with something simple. My goal is to find out how the 50 most popular subreddits are related to each other. The idea behind it is that users usually write comments in subreddits which are close to their personal interests, meaning that a user who is active in the <a href="http://www.reddit.com/r/StarWars">/r/StarWars</a> subreddit is probably also active in the <a href="http://www.reddit.com/r/firefly">/r/firefly</a> subreddit because both categories fit his or her interest in science fiction.</p>

<p><a href="/assets/images/rd-comments/rd_comments_net_hd.png"><img src="/assets/images/rd-comments/rd_comments_net_hd.png" alt="Subreddit Network" style="max-height:640px; max-width:640px" /></a></p>

<p>Based on this assumption, my approach was to look at all 1.2 million unique users who posted a comment in at least one of the top 50 subreddits during January 2016. To calculate the strength of the relationship between the subreddits, I used multiple logistic regression models which for example can tell us how much the probability of a Redditor posting a comment in <a href="http://www.reddit.com/r/StarWars">/r/StarWars</a> increases if he or she also posted a comment in <a href="http://www.reddit.com/r/firefly">/r/firefly</a>. The bigger this number is, the more closely related those subreddits are to each other. The network graph above is a visualization of these results. A bigger dot stands for a larger number of connections to its neighbors.</p>

<p>Looking at the graph, we can identify four major groups of subreddits:</p>

<ol>
  <li><strong>News &amp; Science</strong>: <a href="http://www.reddit.com/r/worldnews">/r/worldnews</a>, <a href="http://www.reddit.com/r/science">/r/science</a>, <a href="http://www.reddit.com/r/space">/r/space</a>, <a href="http://www.reddit.com/r/futurology">/r/futurology</a>, …</li>
  <li><strong>Entertainment</strong>: <a href="http://www.reddit.com/r/movies">/r/movies</a>, <a href="http://www.reddit.com/r/television">/r/television</a>, <a href="http://www.reddit.com/r/music">/r/music</a>, <a href="http://www.reddit.com/r/books">/r/books</a>, …</li>
  <li><strong>Visual Content</strong>: <a href="http://www.reddit.com/r/funny">/r/funny</a>, <a href="http://www.reddit.com/r/pics">/r/pics</a>, <a href="http://www.reddit.com/r/aww">/r/aww</a>, <a href="http://www.reddit.com/r/creepy">/r/creepy</a>, …</li>
  <li><strong>Textual Content</strong>: <a href="http://www.reddit.com/r/showerthought">/r/showerthought</a>, <a href="http://www.reddit.com/r/askreddit">/r/askreddit</a>, <a href="http://www.reddit.com/r/tifu">/r/tifu</a>, <a href="http://www.reddit.com/r/lifeprotips">/r/lifeprotips</a>, …</li>
</ol>

<p>The subreddit <a href="http://www.reddit.com/r/todayilearned">/r/todayilearned</a> doesn’t belong to any particular group because it’s somewhat popular among all users. This analysis doesn’t go into great detail, but I think it’s nevertheless interesting to see that the groups of subreddit seem to make sense and can be interpreted. For example, it doesn’t sound wrong that users who enjoy commenting on topics about space are also interested in science.</p>

<p>Additionally, I also made a table from the same data. The software programs I used to create both graphs are <a href="https://gephi.org/">Gephi</a> and <a href="https://public.tableau.com/s/">Tableau</a> respectively. A blue square stands for a positive correlation coefficient whereas a red square represents the opposite.  You can open the full table by clicking on the graph below:</p>

<p><a href="https://public.tableau.com/profile/curious.gnu#!/vizhome/RedditComments-LogitRegressioncuriousgnu_com/Sheet1"><img src="/assets/images/rd-comments/rd_table.png" alt="Subreddit Table" /></a></p>

<p>Admittedly, these aren’t exactly groundbreaking results, but it was real fun to try out some statistical methods on this huge amount of data. I’m currently testing how I can use this data source for an article about text analysis.</p>

	  ]]></description>
	</item>

	<item>
	  <title>Penny Auctions - How to sell a $180 tablet for $7,264</title>
	  <link>//penny-auctions</link>
	  <author>CuriousGnu</author>
	  <pubDate>2016-04-04T20:00:00+02:00</pubDate>
	  <guid>//penny-auctions</guid>
	  <description><![CDATA[
	     <p>Unless you use an ad blocker, you probably notice ads for penny auction sites from time to time. They usually advertise with sketchy messages like <em>“iPhone sold for $14.21.”</em> They can sell iPhones such low prices because of their unusual auction system where each bid increases the auction price by only one cent. This works because unlike eBay each bid costs money (e.g., $0.40) no matter if you end up winning the auction or not. You have to be the highest bidder when the clock runs out to win the auction. The problem is that each bid adds ten seconds to the countdown which gives other bidders the time to counter your bid.</p>

<p><img src="/assets/images/penny-auctions/mockup_paad.jpg" alt="Penny Auction" /></p>

<p>Penny auction sites are not something new and have been criticized a lot, which makes you wonder how they are still in business. Even though there are many news articles about online penny auctions, I did not find any numbers or statistics which would support the criticism. So I started to collect information on my own, to get a better understanding how these auctions work in reality. What I found exceeded all my expectations.</p>

<p>On <a href="https://en.wikipedia.org/wiki/Beezid">beezid.com</a>, one of the bigger penny auction sites, a single $180 tablet generated 18,160 bids, worth of over <strong>$7,200</strong>, from 56 users. Shockingly nearly half of those bids came from just one person who lost approximately lost <strong>$3,500</strong> in just two hours (see graph below). The winner of this auction only spent 80 cents on his or her two bids. In the second half of this blog post, I will outline the data collection process. To protect the users’ privacy, I replaced the real usernames with chemical elements.</p>

<p><a href="/assets/images/penny-auctions/money_spent_hd.png"><img src="/assets/images/penny-auctions/money_spent.png" alt="Money Spent on Bids" /></a></p>

<h4 id="why-are-penny-auctions-even-popular">Why are penny auctions even popular?</h4>
<p>The big ‘achievement’ of penny auction sites is that they successfully turned an unappealing type of an all-pay auction into an online game which makes people believe that they could make a profit.  The way beezid.com does this is actually quite clever. First, they give their users many different options how they can make a bid. Users can, for example, use automatic bidding bots, take advantage of price limits above which the auction price will not rise, and purchase a wide variety of other supposedly useful add-ons. Second, they try to hide all information that could reveal how many people are participating in the auction and how much they already spent in total.</p>

<p>One way they do this is by their default 10% price limit, which means the auction price will never rise above 10% of the retail price, no matter how many people bid on it. Another strange rule is that bids do not always increase the auction price by one cent but can also lower it by some amount. These modifications of the system make the auction price more or less meaningless because you can no longer assume that, for example, a price of $1.10 is a result of 110 unique bids. Many articles about penny auctions (e.g., <a href="http://www.consumerreports.org/cro/2011/12/with-penny-auctions-you-can-spend-a-bundle-but-still-leave-empty-handed/index.htm">consumerreports.org</a>) do not account for these rules and falsely assume a fixed price increase. The graph below shows the development of the auction price of the $180 tablet over time. If we assume, that each bid would have increased the price by always one cent, the final auction price would have been <strong>$181.60</strong>.</p>

<p><a href="/assets/images/penny-auctions/auction_price_hd.png"><img src="/assets/images/penny-auctions/auction_price.png" alt="Auction Price" /></a></p>

<p>This lack of transparency combined with the many different bidding options creates a system which gives users the illusion that they could make a profit with the right strategies.</p>

<h4 id="how-to-monitor-penny-auctions">How to monitor penny auctions?</h4>
<p>Despite the site’s efforts to keep their users in the dark, it is possible to archive the complete bidding history of an auction, if you monitor it from its start. To collect the data for this blog post, I wrote a simple <a href="https://gist.github.com/CuriousGnu/cf558e8ca8cbbd491619a9e1940682c3">script</a> in Python which automatically saves the all bidding information of a <a href="https://en.wikipedia.org/wiki/Beezid">beezid.com</a> auction. If you want to try it out yourself, you only have to change the auction id and update the request header. You can easily get this information with the Chrome DevTools. In case, you have a slow or unstable internet connection I highly recommend running the script on a server (e.g., <a href="https://www.digitalocean.com/pricing/">Digital Ocean</a>). It should be noted that the script behaves like a regular browser and does not bypass any server-side security or content protection mechanisms.</p>

<h4 id="arent-penny-auctions-unregulated-gambling">Aren’t penny auctions unregulated gambling?</h4>
<p>One of the most important parts of the business models of penny auction sites is that what they are doing is not considered gambling. I am not a lawyer, so I will not attempt to question the legality of their operations. Their central argument is that penny auctions require skill and are therefore exempt from the <a href="https://en.wikipedia.org/wiki/Unlawful_Internet_Gambling_Enforcement_Act_of_2006">Unlawful Internet Gambling Enforcement Act</a>, which is the same legal loophole daily fantasy sports sites like <a href="https://en.wikipedia.org/wiki/FanDuel">FanDuel</a> or <a href="https://en.wikipedia.org/wiki/DraftKings">DraftKings</a> use. Personally, I cannot see how any skill could increase your chances of winning an auction. The data clearly shows that you are bidding against an unknown number of users who sometimes act extremely irrational. Even if you had a comprehensive database of previous auctions, you probably would not be able to predict how far a specific user will go. If someone has already spent over $3,000 on a $180 tablet, what could stop them aside from his or her credit card limit? It would be really interesting to hear an official explanation of how skill is even a factor in this game.</p>

<p>If you have any questions about this blog post, feel free to contact me by <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#099;&#111;&#110;&#116;&#097;&#099;&#116;&#064;&#099;&#117;&#114;&#105;&#111;&#117;&#115;&#103;&#110;&#117;&#046;&#099;&#111;&#109;">email</a>.</p>

	  ]]></description>
	</item>

	<item>
	  <title>How positive are your tweets?</title>
	  <link>//tweetanalyzer</link>
	  <author>CuriousGnu</author>
	  <pubDate>2016-03-16T22:42:00+01:00</pubDate>
	  <guid>//tweetanalyzer</guid>
	  <description><![CDATA[
	     <p>In this blog post, I would like to present you <a href="https://tweetanalyzer.net">tweetanalyzer.net</a>, a small project of mine, where you can do a sentiment analysis of your tweets or the ones of any other Twitter user. The goal was to create a fun website which uses text analysis to determine how positive someone’s tweets are.</p>

<p><a href="https://tweetanalyzer.net"><img src="/assets/images/tweetanalyzer/screen.png" alt="TweetAnalyzer.net" /></a></p>

<p>After you signed in with your Twitter account, you can type in any username and the website will automatically download the latest 200-300 tweets and calculate a positivity and vulgarity score for each of them. The method used to do this is very simple. The backend uses the AFINN word list from <a href="http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010">Finn Årup Nielsen</a>, which contains nearly 2,500 words rated for their positivity from -5 (negative) to +5 (positive). After removing stop words, hashtags, and URLs the script looks up every word in this list and calculates the sum for each tweet. If this sum is over zero, the tweet will be marked as positive if it’s under zero as negative. The overall positivity score for a user is then calculated as follows:</p>

<p><img src="/assets/images/tweetanalyzer/positivity_score.gif" alt="PSF" /></p>

<p>Admintingly this approach has its weakness and is not as sophisticated as many other methods classification technique out there. For example, it can not understand the context of a tweet and how a specific word (e.g., <em>sick</em>) is used. I experimented more complex Python libraries for text analysis, but unfortunately they did not run well on the Google App Engine platform if you plan to analyse thousands of tweets but only have a limited budget. I am sure that it can be done without a problem but since I never used it before and did not want to spend too much time on it, I decided to use the simple solution which I believe is still sufficient for such a task.</p>

<p>To determine how vulgar a tweet is, the software uses a similar word list based method. If a tweet contains a word which is in this list, it will be marked as <em>vulgar</em>. The lack of content-awareness will, of course, lead to some mistakes. For example, news articles about <em>sex trafficking</em> or <em>rape</em> will be falsely classified as <em>vulgar</em>. So please do not take the results too seriously and rather tweet something positive about it! Feel free to <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#099;&#111;&#110;&#116;&#097;&#099;&#116;&#064;&#099;&#117;&#114;&#105;&#111;&#117;&#115;&#103;&#110;&#117;&#046;&#099;&#111;&#109;">contact me</a> if you suggestions or questions.</p>

<p>You can find the project here: <a href="https://tweetanalyzer.net">https://tweetanalyzer.net</a></p>

	  ]]></description>
	</item>

	<item>
	  <title>What are the most common words in TV shows?</title>
	  <link>//tv-show-word-clouds</link>
	  <author>CuriousGnu</author>
	  <pubDate>2016-02-27T13:42:00+01:00</pubDate>
	  <guid>//tv-show-word-clouds</guid>
	  <description><![CDATA[
	     <p>Nowadays, there are some many great shows on television or available for streaming. After binge-watching the first season of Jessica Jones, I knew that I had to post something about this topic. I have to admit that I never spend much time on text analysis, so I decided to start with the basics. The first thing I did was to create simple word clouds based on the subtitles of <em>Jessica Jones</em>, <em>The Blacklist</em>, and <em>TBBT</em>. They are an easy way to visualize how frequently certain words are used in a text or TV show.</p>

<p>Besides this basic type of word cloud, I also created a slightly different version of it. For this, I did not just count the words but also looked how close they are to a character’s name. This means that the more frequent a word is used in combination with a character’s name, the bigger it is in the word cloud.</p>

<p><a href="/assets/images/tv-wordcloud/wordcloud_jessicajones_hd.png"><img src="/assets/images/tv-wordcloud/wordcloud_jessicajones.png" alt="Jessica Jones Wordcloud" title="Jessica Jones Wordcloud" /></a></p>

<p><a href="/assets/images/tv-wordcloud/wordcloud_blacklist_hd.png"><img src="/assets/images/tv-wordcloud/wordcloud_blacklist.png" alt="The Blacklist Wordcloud" title="The Blacklist Wordcloud" /></a></p>

<p><a href="/assets/images/tv-wordcloud/wordcloud_tbbt_hd.png"><img src="/assets/images/tv-wordcloud/wordcloud_tbbt.png" alt="TBBT Wordcloud" title="TBBT Wordcloud" /></a></p>

<p>I created all those word clouds with the R package <a href="https://cran.r-project.org/web/packages/wordcloud/index.html"><em>wordcloud</em></a> which makes this job so easy. Besides that, the <em>findAssocs()</em> function from the <a href="https://cran.r-project.org/web/packages/tm/index.html"><em>tm</em></a> package was used calculate the often a word is used close to a character’s name. For the second type of word clouds, I used the correlation coefficients from this function as a weighting factor.</p>

<p>Before doing that, I replaced all the different versions of a name (e.g., Lizzy, Liz, Keen) with the name you can see in the center of the word clouds. In case you are wondering why there are not always complete words in the clouds, it is because of a process called <em>stemming</em> which reduces words to their word stem. The idea behind it is to group the different forms of a word together instead of treating every single form as an own term.</p>

<p>I hope you like this post. Please feel free to contact me if you have any questions or suggestions.</p>

	  ]]></description>
	</item>

	<item>
	  <title>Who sells LSD on the Darktnet?</title>
	  <link>//cryptomarkets-lsd-sales</link>
	  <author>CuriousGnu</author>
	  <pubDate>2016-01-16T12:10:00+01:00</pubDate>
	  <guid>//cryptomarkets-lsd-sales</guid>
	  <description><![CDATA[
	     <p>I recently got my hands on Diana S. Dolliver’s <a href="http://dx.doi.org/10.1016/j.drugpo.2015.01.008" target="_blank">paper</a> about drug dealing on the Tor network, a hidden, uncensored network which can only be accessed by <a href="https://www.torproject.org/" target="_blank">special software</a>. I got interested this topic because it allows us to analyse transactions which used to be hidden and nearly impossible to observe.</p>

<p>Besides the many legitimate use cases of the Tor network, it also provides together with Bitcoin the technology most <em>cryptomarkets</em> use. Tor allows the side administrators to host their sites anonymously without the fear of getting arrested immediately by the FBI. Cryptomarkets function similar to eBay, in the sense that they do not sell drugs directly but provide a platform where other vendors can sell their products for a small fee. The screenshot below shows how such a site looks from a buyers perspective. I chose to take a close look at the <em>AlphaBay Market</em>, one of the biggest platforms in December 2015.</p>

<p><a href="/assets/images/darknet-lsd/screenshot.png"><img src="/assets/images/darknet-lsd/screenshot.png" alt="AlphaBay Market Screenshot" title="AlphaBay Market Screenshot" /></a></p>

<h5 id="how-to-analyse-the-alphabay-market">How to analyse the AlphaBay Market?</h5>

<p>If you access one of the larger cryptomarkets, you will probably see thousands of different offerings ranging from prescription drugs to illegal weapons. The fact that anonymous users claim to sell those products, of course, does not say anything about the actual demand and volume of the sales. This is the point where it gets interesting because the <em>AlphaBay Market</em> has a revealing feedback system which lists not only lists all products sold but also their prices, and purchase dates.</p>

<p>Assuming a constant feedback ratio, we can use this data to observe the early development of the market from only one snapshot. To automatically download the customer feedbacks I wrote a scraper in Python, which provided me with the dataset for the further analysis.</p>

<h5 id="how-much-lsd-is-sold">How much LSD is sold?</h5>

<p>Unfortunately, the scraping process of a website hidden in the Tor network is a quite slow. Therefore, I decided only to download information related to LSD sales. The diagram below shows last year’s LSD sales for the weeks 13 to 50:</p>

<p><a href="/assets/images/darknet-lsd/plot_hd.png"><img src="/assets/images/darknet-lsd/plot.png" alt="LSD Sales" title="LSD Sales" /></a></p>

<p>During November 2015, approximately <strong>$215,000</strong> worth of LSD was sold on the AlphaBay Market. I do not have a definitive explanation for the sales increase following week 44, but my guess is that problems with competing cryptomarkets drove new sellers to this platform and their existing customers followed. This theory is supported by the fact that already established vendors like <em>Lyseric025</em> were not able to significantly increase their revenue during this time.</p>

<h5 id="who-are-the-dealers">Who are the dealers?</h5>

<p>To better get a better understanding of who those LSD vendors are, we need to look at their other sales also:</p>

<p><a href="/assets/images/darknet-lsd/table_hd.png"><img src="/assets/images/darknet-lsd/table.png" alt="LSD Vendors" title="LSD Vendors" /></a></p>

<p>We can see in the table above that the leading LSD seller on the AlphaBay Market was <em>Lyseric025</em>, who had a market share of 29% in November 2015 and generated a revenue of $65,438. Only 3% of it came from B2B sales (sale over $200) which suggest a strong focus on the end customer.</p>

<p>Interestingly most sellers on this list made the majority of their revenue with LSD. One of the reasons for this could be that the LSD is supposedly produced by only a <a href="http://www.justice.gov/archive/ndic/pubs25/25921/25921p.pdf" target="_blank">few producers</a> which wholesalers are probably only interested in selling in large volumes. This would mean that a specialized vendor could increase its profit by using economies of scale. I wonder if there is a similar specialization in other more decentralized drug markets.</p>

	  ]]></description>
	</item>

	<item>
	  <title>What's the age difference in movies?</title>
	  <link>//imdb-age-gap</link>
	  <author>CuriousGnu</author>
	  <pubDate>2015-11-04T13:21:00+01:00</pubDate>
	  <guid>//imdb-age-gap</guid>
	  <description><![CDATA[
	     <p>In response to my <a href="/imdb-age-distribution">last post</a> about the distribution of ages of actresses and actors, a Reddit user suggested investigating further the age difference between female and male movie stars. In my first simple analysis, I only compared the average ages of both groups in the overall sample. Now, the next step is to check on a movie basis if there is an age difference between the leading actress and her co-star.</p>

<h5 id="methods">Methods</h5>
<p>First I needed to identify the two leads of each film (2000-2015, with 10,000+ votes) to calculate the age difference. Unfortunately, there is no direct way to get this information from the IMDb dataset. I, therefore, used a quick-and-dirty approve and chose the first actress and actor in the IMDb cast credits as leads. If I did not find both an actress and an actor in the first three entries (in credits order), I removed the whole movie from the sample. With this method, I was able to get the leading actresses and actors of 1,201 movies. Note that they are not necessarily movie couples. I am aware that a manually selected sample would have been better but considering the simplicity of my approach; I think that the results are acceptable for this analysis. Feel free to check it yourself <a href="/assets/data/age_gap_2015.csv">here</a>.</p>

<h5 id="results">Results</h5>
<p>When we look at the box-plot of the age differences, we can see that on average the leading actor is <strong>five years older</strong> than his female co-star. It also tells us that in approximately 75% of all movies in our sample have an older male lead. Based on this data, the answer to the question from the title is <em>“Yes”</em> even though the age gap might be not as big as expected.</p>

<p><a href="/assets/images/age-gap/plot_2.png"><img src="/assets/images/age-gap/plot_2.png" alt="Age Gap Boxplot" title="Age Gap Boxplot" /></a></p>

<p>Finally, I also created a density plot which summarizes all data points in a nice looking, colorful graph. The area below the gray dashed line stands for all the movies in which the leading actor is older :</p>

<p><a href="/assets/images/age-gap/plot_1_hd.png"><img src="/assets/images/age-gap/plot_1.png" alt="Age Gap Plot" title="Age Gap Plot" /></a></p>

	  ]]></description>
	</item>

	<item>
	  <title>Actresses Are on Average 7 Years Younger</title>
	  <link>//imdb-age-distribution</link>
	  <author>CuriousGnu</author>
	  <pubDate>2015-11-02T21:10:00+01:00</pubDate>
	  <guid>//imdb-age-distribution</guid>
	  <description><![CDATA[
	     <p>The IMDb dataset is a great source for everyone loves movies and numbers. After I had figured out how to <a href="http://imdbpy.sourceforge.net/support.html" target="_blank">import the data</a> into a local SQL database, the first thing I did was to look at the age of the actresses and actors during filming.</p>

<p>It turns out that the average actress is, with a median age of 32, <strong>seven years younger</strong> than her male counterpart who is on average 39 years old. The diagram below shows that the distribution of ages of actresses is more skewed to the right (γ1=.912) than those of ages of actors (γ1=.483). This suggests that there is a relatively higher demand for actresses under the age of 35. Interestingly we do not see such an apparent age preference in the casting of actors.</p>

<p><a href="/assets/images/age-dist/plot_hd.png"><img src="/assets/images/age-dist/plot.png" alt="Age Distribution Plot" title="Age Distribution Plot" /></a></p>

<p>The sample consists of all roles played by actresses (n=21,551) and actors (n=50,165) in U.S. movie released between 2000 and 2015, with more than 10,000 IMDb-Votes. By the way, in the United States, the median age of females and males is 39.2 years and 36.5 years respectively (<a href="https://www.cia.gov/library/publications/the-world-factbook/geos/us.html">The World Factbook, 2015</a>).</p>

<p><small>Photo: “California Movie Theater Berkeley” by <a href="https://flic.kr/p/aM4GwX">Russell Mondy</a> is licensed under CC BY-NC 2.0</small></p>

	  ]]></description>
	</item>


</channel>
</rss>
