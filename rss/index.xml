<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:webfeeds="http://webfeeds.org/rss/1.0">
  <channel>
    <title>CuriousGnu.com - Articles</title>
    <webfeeds:analytics id="UA-74290781-1" engine="GoogleAnalytics"/>
    <description>Numbers, Graphs, and Apple Strudels</description>
    <link>https://www.curiousgnu.com/</link>
    
      
      <item>
        <title>Analyzing Subtitles - Chick Flick or Guy Movie?</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/subtitles-target-audience/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;Romantic comedy or action movie? While this cliché certainly oversimplifies the differences in movie preferences between men and women, it is common knowledge that certain movie genres primarily target one gender. The colloquial term &lt;em&gt;chick flick&lt;/em&gt; refers to movies targeted to a female audience whereas &lt;em&gt;guy movies&lt;/em&gt; are mainly aimed at male viewers. For example, a study found that female viewers have a stronger preference for movies with happier themes than their male counterparts (&lt;cite&gt;&lt;a href=&quot;http://dx.doi.org/10.1027/1864-1105.20.3.97&quot;&gt;Banerjee et al., 2008&lt;/a&gt;&lt;/cite&gt;).&lt;/p&gt;

&lt;p&gt;For this post, my goal is to investigate how &lt;em&gt;chick flicks&lt;/em&gt;, and &lt;em&gt;guy movies&lt;/em&gt; differ regarding the words spoken. As obtaining the original scripts is rather difficult, I decided to analyze the movies’ subtitles instead, which were available from &lt;cite&gt;&lt;a href=&quot;https://www.amazon.com/s/?rh=n%3A7589478011%2Cn%3A2858905011%2Cp_n_consumption_type%3A9052272011%2Cp_n_feature_fourteen_browse-bin%3A2654454011%2Cp_n_ways_to_watch%3A12007867011%2Cp_n_feature_twelve_browse-bin%3A5824772011&quot;&gt;Amazon Video&lt;/a&gt;&lt;/cite&gt;. At the end of this post, you will find a brief description of how subtitles can be downloaded from the Amazon website. Since subtitles are protected by copyright, please understand that I cannot share the files that I used in this project.&lt;/p&gt;

&lt;h4 id=&quot;data&quot;&gt;Data&lt;/h4&gt;
&lt;p&gt;I downloaded the subtitles of the &lt;strong&gt;top 1,000&lt;/strong&gt; &lt;cite&gt;&lt;a href=&quot;https://www.amazon.com/s/?rh=n%3A7589478011%2Cn%3A2858905011%2Cp_n_consumption_type%3A9052272011%2Cp_n_feature_fourteen_browse-bin%3A2654454011%2Cp_n_ways_to_watch%3A12007867011%2Cp_n_feature_twelve_browse-bin%3A5824772011&quot;&gt;Amazon Video bestsellers&lt;/a&gt;&lt;/cite&gt; (7th March, 2017) and gathered additional information from &lt;cite&gt;&lt;a href=&quot;https://www.imdb.com&quot;&gt;IMDb&lt;/a&gt;&lt;/cite&gt;. To determine whether a film is targeted at a male or female audience, I used the proportion of women votes on the IMDb score. In this sample, women make up on average about &lt;strong&gt;one-quarter&lt;/strong&gt; of all gender votes.&lt;/p&gt;

&lt;p&gt;25% of the movies with the lowest share of female votes were defined as &lt;em&gt;guy movies&lt;/em&gt; while the top 25% were defined as &lt;em&gt;chick flicks&lt;/em&gt;. Next, I removed movies with less than 1,000 total votes to ensure a fair comparison. Considering the rather vague definition of these two terms, I believe that this approach is sufficient for this context. The following table shows the top ten movies for each category:&lt;/p&gt;

&lt;table style=&quot;font-size:14px&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;#&lt;/th&gt;
      &lt;th&gt;Chick Flicks (% female votes)&lt;/th&gt;
      &lt;th&gt;Guy Movies (% female votes)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Never Back Down: No Surrender (3.8%)&lt;/td&gt;
      &lt;td&gt;Northanger Abbey (81.7%)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Justice League Dark (4.8%)&lt;/td&gt;
      &lt;td&gt;Persuasion (72.3%)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Dragon Ball Z: Resurrection (5.1%)&lt;/td&gt;
      &lt;td&gt;The Last Song (65.4%)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Batman: Bad Blood (5.2%)&lt;/td&gt;
      &lt;td&gt;The Princess Diaries 2 (64.1%)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Justice League: Throne of Atlantis (5.6%)&lt;/td&gt;
      &lt;td&gt;Something Borrowed (62.6%)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;Kill Command (5.7%)&lt;/td&gt;
      &lt;td&gt;Letters To Juliet (62.5%)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Justice League vs. Teen Titans (6.3%)&lt;/td&gt;
      &lt;td&gt;Ever After: A Cinderella Story (60.7%)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;Headshot (6.4%)&lt;/td&gt;
      &lt;td&gt;Little Women (59.7%)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;We Were Soldiers (6.6%)&lt;/td&gt;
      &lt;td&gt;27 Dresses (58.5%)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Predator (6.7%)&lt;/td&gt;
      &lt;td&gt;The Young Victoria (58.2%)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;analysis&quot;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;First, I used the ‘&lt;cite&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/wordcloud/&quot;&gt;&lt;em&gt;wordcloud&lt;/em&gt;&lt;/a&gt;&lt;/cite&gt;’ package in R to plot a comparison word cloud, which highlights the words that were heavily used in one of the two categories. To remove movie specific words, such as character names, I dropped all words from the sample that were not in at least three different movie subtitles.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/subtitles-target-audience/wordcloud.png&quot;&gt;&lt;img src=&quot;/assets/images/subtitles-target-audience/wordcloud.png&quot; alt=&quot;Subtitles Wordcloud&quot; style=&quot;max-width: 600px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The more frequent use of profane language in &lt;em&gt;guy movies&lt;/em&gt; and the higher use of positive words in &lt;em&gt;chick flicks&lt;/em&gt; is in line with the assumption that women prefer relationship movies while men are more interested in thriller and action genres. A quick sentiment analysis performed through the ‘&lt;cite&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/sentimentr/&quot;&gt;&lt;em&gt;sentimentr&lt;/em&gt;&lt;/a&gt;&lt;/cite&gt;’ R-package confirms that movies targeted at a female audience are slightly more positive than movies produced for a male audience.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/subtitles-target-audience/sentiment.png&quot;&gt;&lt;img src=&quot;/assets/images/subtitles-target-audience/sentiment.png&quot; alt=&quot;Subtitles Sentiment&quot; style=&quot;max-width: 500px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These results show that the use of certain words differs significantly between the two categories. Furthermore, these findings suggest that we can use subtitles to determine whether a movie is more likely to target a male or female audience. As this is a traditional text classification problem, a wide variety of machine learning algorithms exist, such as &lt;cite&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Support_vector_machine&quot;&gt;support vector machines&lt;/a&gt;&lt;/cite&gt;, &lt;cite&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;naive Bayes classifiers&lt;/a&gt;&lt;/cite&gt;, or &lt;cite&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Boosting_(machine_learning)&quot;&gt;boosting classifiers&lt;/a&gt;&lt;/cite&gt;, which could be applied.&lt;/p&gt;

&lt;p&gt;Even though some of these algorithms would probably be more accurate and robust for this task, I used a &lt;cite&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Decision_tree_learning&quot;&gt;classification tree&lt;/a&gt;&lt;/cite&gt; model (‘&lt;cite&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/rpart/index.html&quot;&gt;&lt;em&gt;rpart&lt;/em&gt;&lt;/a&gt;&lt;/cite&gt;’, R-package) because it produces results that are easy to understand and apply. The goal of this method is to derive a set of rules from input variables that can predict which class an item belongs to. In this case, the input variables for each movie are the numbers of occurrences of each word stem. The target category is either &lt;em&gt;chick flick&lt;/em&gt; or &lt;em&gt;guy movie&lt;/em&gt;. The following tree is based on a randomly selected training set.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/subtitles-target-audience/tree.png&quot;&gt;&lt;img src=&quot;/assets/images/subtitles-target-audience/tree.png&quot; alt=&quot;Subtitles Regression Tree&quot; style=&quot;max-width: 500px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;According to this model, if the word stem ‘&lt;strong&gt;&lt;em&gt;love&lt;/em&gt;&lt;/strong&gt;’ appears more than 11 times in the subtitles and ‘&lt;strong&gt;&lt;em&gt;gun&lt;/em&gt;&lt;/strong&gt;’ is only used once, the movie is most likely targeted at women. However, if ‘&lt;strong&gt;&lt;em&gt;love&lt;/em&gt;&lt;/strong&gt;’ occurs no more than 11 times in the subtitles and ‘&lt;strong&gt;&lt;em&gt;hell&lt;/em&gt;&lt;/strong&gt;’ at least once, the movie is probably produced for a male audience. When applied to the test set, this model predicts &lt;strong&gt;82%&lt;/strong&gt; of the categories correctly. This is significantly higher than the &lt;strong&gt;54%&lt;/strong&gt; no information rate, which is the percentage of the largest class in the training set.&lt;/p&gt;

&lt;p&gt;Overall, I think it is quite interesting to see how the results support the assumption about typical &lt;em&gt;chick flicks&lt;/em&gt; and &lt;em&gt;guy movies&lt;/em&gt;. Knowing that gender specific movies are a controversial topic for some people, I hope that no one is offended by this post or my choice of words as this is unintentional.&lt;/p&gt;

&lt;h5 id=&quot;download-subtitles-from-amazon-video&quot;&gt;Download Subtitles from Amazon Video&lt;/h5&gt;
&lt;p&gt;Amazon Video offers high quality subtitles that can be downloaded in the &lt;cite&gt;&lt;a href=&quot;https://www.w3.org/TR/2006/CR-ttaf1-dfxp-20061116/&quot;&gt;&lt;em&gt;DFXP format&lt;/em&gt;&lt;/a&gt;&lt;/cite&gt; using Google Chrome’s Developer Tools. To get the subtitles, you need to start streaming the requested movie. Therefore, being a Prime member is very helpful as it includes free access to a large portion of the video content. Otherwise, gathering the subtitles for a large sample of movies could become rather expensive.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/subtitles-target-audience/screenshot.png&quot;&gt;&lt;img src=&quot;/assets/images/subtitles-target-audience/screenshot.png&quot; alt=&quot;Subtitles Screenshot&quot; style=&quot;max-width: 668px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Visit the movie’s product page (e.g. &lt;cite&gt;&lt;a href=&quot;https://www.amazon.com/dp/B01CUVU7DQ/&quot;&gt;amazon.com/dp/B01CUVU7DQ&lt;/a&gt;&lt;/cite&gt;)&lt;/li&gt;
  &lt;li&gt;Open &lt;em&gt;Developer Tools&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Select the &lt;em&gt;Network&lt;/em&gt; tab&lt;/li&gt;
  &lt;li&gt;Click the &lt;em&gt;Watch Now&lt;/em&gt; button&lt;/li&gt;
  &lt;li&gt;Type ‘subtitle’ into the search bar&lt;/li&gt;
  &lt;li&gt;Select &lt;em&gt;GetPlaybackResources?…&lt;/em&gt; and open the &lt;em&gt;Preview&lt;/em&gt; tab&lt;/li&gt;
  &lt;li&gt;Open the URL under [subtitleURLs &amp;gt; 0 &amp;gt; url]&lt;/li&gt;
  &lt;li&gt;Done!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you have any questions or concerns about this post, feel free to write me an &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;email&lt;/a&gt;.&lt;/p&gt;


          </description>
        <pubDate>Wed, 22 Mar 2017 03:00:00 +0100</pubDate>
        <link>https://www.curiousgnu.com/subtitles-target-audience</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/subtitles-target-audience</guid>
      </item>
      
    
      
      <item>
        <title>Text Analysis of YouTube Comments</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/youtube-comments/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;According to &lt;cite&gt;&lt;a href=&quot;http://www.alexa.com/topsites&quot;&gt;Alexa.com&lt;/a&gt;&lt;/cite&gt;, an Amazon subsidiary that analysis web traffic, YouTube is the world’s most popular social media site. Its user numbers even exceed those of web giants such as Facebook or Wikipedia. Over the past twelve years, YouTube has become a diverse platform where users can find and watch videos in a wide variety of genres, from cute cats to recorded university lectures.&lt;/p&gt;

&lt;p&gt;Even though user comments are an integral part of the YouTube community, the comment section is also infamous as a home for trolls, negativity, and insults. For me, the broad acceptance of YouTube makes its user comments an interesting subject that’s worth a closer look. My plan for this post is to use text analysis to find out more about YouTube comments and determine whether they differ among certain categories.&lt;/p&gt;

&lt;h4 id=&quot;dataset&quot;&gt;Dataset&lt;/h4&gt;
&lt;p&gt;To download a large set of YouTube comments, I used a Python script that uses the official &lt;cite&gt;&lt;a href=&quot;https://developers.google.com/youtube/v3/&quot;&gt;YouTube API&lt;/a&gt;&lt;/cite&gt;, which (fortunately) offers generous API limits, allowing us to gather hundreds of thousands of individual comments. For this analysis, I decided to download only comments that refer to one of twenty selected channels in one of the following four categories: comedy, science, TV, and news &amp;amp; politics (see table below). I used Socialblade’s &lt;cite&gt;&lt;a href=&quot;http://socialblade.com/youtube/top/100&quot;&gt;YouTube top list&lt;/a&gt;&lt;/cite&gt; as a guideline. However, I also took the liberty of excluding some channels that didn’t fit the category or targeted non-English-speaking viewers.&lt;/p&gt;

&lt;table style=&quot;font-size:14px&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Category&lt;/th&gt;
      &lt;th&gt;Channels&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;comedy&lt;/td&gt;
      &lt;td&gt;PewDiePie, SMOSH, CollegeHumor, FailArmy, JennaMarbles&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;science&lt;/td&gt;
      &lt;td&gt;AsapSCIENCE, SciShow, Numberphile, ScienceChannel, Veritasium&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TV&lt;/td&gt;
      &lt;td&gt;Jimmy Fallon, Conan, James Corden,  Jimmy Kimmel, TheEllenShow&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;news &amp;amp; politics&lt;/td&gt;
      &lt;td&gt;TYT, ABCNews, CNN, Infowars, Vox&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Of these channels, the 25 most-watched videos between the years 2015 and 2016 were identified. For each of these videos, up to 500 of the most relevant comments were downloaded. Users responding to other users were ignored to ensure that each comment was an independent contribution. After letting the Python script run for about 1 hour, I ended up with a dataset containing just over 350,000 comments. You can find the script and the R script for the following analysis here: &lt;a href=&quot;https://s3.amazonaws.com/cgcdn-misc/yt_comments_code.zip&quot;&gt;yt_comments_code.zip&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;analysis&quot;&gt;Analysis&lt;/h4&gt;
&lt;p&gt;For the analysis, I switched from Python to R so that I could use the &lt;cite&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/quanteda/&quot;&gt;quanteda&lt;/a&gt;&lt;/cite&gt; package, a handy toolset for quantitative text analysis. First, I generated a comparison word cloud for the four previously defined categories. In contrast to a traditional world cloud, in which the font sizes represent the words’ numbers of occurrences, a comparison word cloud illustrates which words are primarily used in specific categories. Therefore, the text size is linked to a word’s maximum deviation from its rate of occurrence in a category and the average across all comments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/youtube-comments/wordcloud.png&quot;&gt;&lt;img src=&quot;/assets/images/youtube-comments/wordcloud.png&quot; alt=&quot;YouTube Comments Wordcloud&quot; style=&quot;max-width: 600px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The comparison word cloud shows that while viewers of news videos commented on political and social issues (e.g., &lt;em&gt;trump&lt;/em&gt;, &lt;em&gt;racist&lt;/em&gt;), comments on videos in the TV category contained more positive words (e.g., &lt;em&gt;love&lt;/em&gt;, &lt;em&gt;funny&lt;/em&gt;, &lt;em&gt;lol&lt;/em&gt;). Furthermore, presumably topic-related keywords characterized comments from the science channels, while the use of profanity appears to be more prevalent in the comment sections of comedy videos.&lt;/p&gt;

&lt;p&gt;These findings lead to the question: do viewers’ contributions significantly differ across the four categories regarding complexity and the use of profanity? A readability measurement like &lt;cite&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/SMOG&quot;&gt;SMOG&lt;/a&gt;&lt;/cite&gt; could be used to measure the complexity of a text. Even though such formulas are sometimes used on text snippets like tweets (e.g., &lt;cite&gt;&lt;a href=&quot;http://time.com/2958650/twitter-reading-level/&quot;&gt;Times&lt;/a&gt;&lt;/cite&gt;), I’m not convinced that it is an appropriate approach because brief internet comments and tweets differ substantially from the newspaper articles and business writing for which most of these measurements were originally designed.&lt;/p&gt;

&lt;p&gt;Therefore, I chose a much simpler approach by using the word count of a comment as an indicator for the comment’s complexity. While this indicator cannot, of course, account for the actual context of a comment, it can be a rough estimate of how much effort a viewer put into commenting on a video. Regarding the use of profanity, a comment was classified as profane if it contained at least one profane word. The source of the used swear-word list is &lt;a href=&quot;http://www.noswearing.com/dictionary&quot;&gt;noswearing.com&lt;/a&gt;. The following bar chart shows the average word length for the four categories and the share of profane comments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/youtube-comments/bar_chart_01.png&quot;&gt;&lt;img src=&quot;/assets/images/youtube-comments/bar_chart_01.png&quot; alt=&quot;YouTube Comments Bar Chart 1&quot; style=&quot;max-width: 500px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We can see that the average comments on videos in the science and news channels are 20 and 24 words respectively, or about twice as long as the comments on videos in the TV and comedy category. With 15% of profane comments, the news channels have the highest share of profanity, whereas the science channels have the lowest rate, at just 5%. Furthermore, a Kruskal-Wallis test and chi-squared test (+post hoc tests) confirmed that the found differences are statistically significant.&lt;/p&gt;

&lt;p&gt;Next, a sentiment analysis was used to determine the polarity of the comments, thereby classifying whether they are positive, negative, or neutral. For this analysis, I employed the &lt;cite&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/syuzhet/&quot;&gt;Syuzhet&lt;/a&gt;&lt;/cite&gt; R-package, which uses a straightforward knowledge-based technique based on lexicons, which are collections of positive and negative words. In the settings, I chose the &lt;cite&gt;&lt;a href=&quot;https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html&quot;&gt;bing&lt;/a&gt;&lt;/cite&gt; lexicon by Minqing Hu and Bing Liu. The next bar chart shows what many percentage of the comments in each category are classified as positive, negative, or neutral.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/youtube-comments/bar_chart_02.png&quot;&gt;&lt;img src=&quot;/assets/images/youtube-comments/bar_chart_02.png&quot; alt=&quot;YouTube Comments Bar Chart 2&quot; style=&quot;max-width: 700px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The news category, at 38%, has not only the highest share of negative comments, but also the lowest percentage (36%) of neutral comments. Considering that political issues are often polarizing topics, this result seems to be reasonable. Interestingly, the TV category, with 32% and 21%, has the highest share of positive and the lowest share of negative comments respectively.&lt;/p&gt;

&lt;h4 id=&quot;summary&quot;&gt;Summary&lt;/h4&gt;
&lt;p&gt;Overall, the analysis of over 350,000 comments shows that their style and content differs substantially across certain YouTube categories. While the average comments on videos about news and politics are longer than comments on other types of content, they also contain significantly more profanity and negativity. Compared to this category, science channels attract comments that are roughly as long, but contain much less profanity. Surprisingly, the comments in the TV category are largely family-friendly and positive. One explanation might be that these channels moderate the comments more strictly than the average news or comedy YouTubers.&lt;/p&gt;

&lt;p&gt;If you have any questions or concerns about this post, feel free to write me an &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;email&lt;/a&gt;.&lt;/p&gt;


          </description>
        <pubDate>Tue, 28 Feb 2017 11:00:00 +0100</pubDate>
        <link>https://www.curiousgnu.com/youtube-comments-text-analysis</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/youtube-comments-text-analysis</guid>
      </item>
      
    
      
      <item>
        <title>Dataset: Yearly Bills of Mortality from 1657 to 1758</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/bills-of-mortality/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;In this blog post, I would like to share the &lt;em&gt;Yearly Bills of Mortality&lt;/em&gt; dataset that I recently generated. The &lt;em&gt;Bills of Mortality&lt;/em&gt; were a mortality statistic for London and were first published in the 17th century. Besides the number of deaths, they also provide information about the cause of death since 1629. The first time I read about these documents was in a &lt;cite&gt;&lt;a href=&quot;http://www.telegraph.co.uk/news/science/science-news/8934045/Historic-medical-records-show-deaths-from-lethargy-and-itch.html&quot;&gt;newspaper article&lt;/a&gt;&lt;/cite&gt; in The Telegraph about the absurdly sounding causes of death such as itch, lethargy, or grief.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/bills-of-mortality/itch.jpg&quot;&gt;&lt;img src=&quot;/assets/images/bills-of-mortality/itch.jpg&quot; alt=&quot;Bills of Mortality - Itch&quot; style=&quot;max-width: 300px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, Google digitized the book &lt;em&gt;“Collection of Yearly Bills of Mortality, from 1657 to 1758 Inclusive”&lt;/em&gt; and made it available through &lt;cite&gt;&lt;a href=&quot;https://books.google.com/books/about/Collection_of_Yearly_Bills_of_Mortality.html?id=3wYAAAAAMAAJ&quot;&gt;Google Books&lt;/a&gt;&lt;/cite&gt;. In addition to that, &lt;cite&gt;&lt;a href=&quot;https://archive.org/details/collectionyearl00hebegoog&quot;&gt;archive.org&lt;/a&gt;&lt;/cite&gt; offers those scans in a variety of formats. Unfortunately, Google’s optical character recognition (OCR) does not work particularly well on historical documents especially if the data is presented in tables, which makes searching the PDF and analyzing the changes in certain numbers difficult.&lt;/p&gt;

&lt;p&gt;Finding those kinds of statistics rather intriguing, I considered the &lt;em&gt;Yearly Bills of Mortality&lt;/em&gt; to be a perfect test case for manual data entry services that I am currently testing. The task was to convert the “Diseases and Casualties” table of 102 pages into an Excel table. In the case of different variants of the same causes of death (e.g., &lt;em&gt;hang’d&lt;/em&gt; and &lt;em&gt;hanged&lt;/em&gt;), a consistent form of spelling was used.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/bills-of-mortality/page.jpg&quot;&gt;&lt;img src=&quot;/assets/images/bills-of-mortality/page.jpg&quot; alt=&quot;Bills of Mortality - Page&quot; style=&quot;max-width: 500px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you are also interested in this subject or just want to explore what killed Londoners between the years 1657 and 1758, you can download the complete dataset here &lt;cite&gt;&lt;a href=&quot;https://s3.amazonaws.com/cgcdn-misc/yearly_bills_of_mortality_1657_1758.zip&quot;&gt;CSV file&lt;/a&gt;&lt;/cite&gt;. Additionally, I uploaded the data to &lt;cite&gt;&lt;a href=&quot;https://public.tableau.com/profile/curious.gnu#!/vizhome/YearlyBillsofMortality1657-1758/Dashboard1&quot;&gt;Tableau Public&lt;/a&gt;&lt;/cite&gt;; you can try it out in the interactive graph below. Please note that I of course cannot guarantee the accuracy of the dataset.&lt;/p&gt;

&lt;div class=&quot;tableauPlaceholder&quot; id=&quot;viz1487713662489&quot; style=&quot;position: relative&quot;&gt;&lt;noscript&gt;&lt;a href=&quot;https:&amp;#47;&amp;#47;www.curiousgnu.com&quot;&gt;&lt;img alt=&quot;Dashboard 1 &quot; src=&quot;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;Ye&amp;#47;YearlyBillsofMortality1657-1758&amp;#47;Dashboard1&amp;#47;1_rss.png&quot; style=&quot;border: none&quot; /&gt;&lt;/a&gt;&lt;/noscript&gt;&lt;object class=&quot;tableauViz&quot; style=&quot;display:none;&quot;&gt;&lt;param name=&quot;host_url&quot; value=&quot;https%3A%2F%2Fpublic.tableau.com%2F&quot; /&gt; &lt;param name=&quot;site_root&quot; value=&quot;&quot; /&gt;&lt;param name=&quot;name&quot; value=&quot;YearlyBillsofMortality1657-1758&amp;#47;Dashboard1&quot; /&gt;&lt;param name=&quot;tabs&quot; value=&quot;no&quot; /&gt;&lt;param name=&quot;toolbar&quot; value=&quot;no&quot; /&gt;&lt;param name=&quot;static_image&quot; value=&quot;https:&amp;#47;&amp;#47;public.tableau.com&amp;#47;static&amp;#47;images&amp;#47;Ye&amp;#47;YearlyBillsofMortality1657-1758&amp;#47;Dashboard1&amp;#47;1.png&quot; /&gt; &lt;param name=&quot;animate_transition&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_static_image&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_spinner&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_overlay&quot; value=&quot;yes&quot; /&gt;&lt;param name=&quot;display_count&quot; value=&quot;yes&quot; /&gt;&lt;/object&gt;&lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt; var divElement = document.getElementById(&#39;viz1487713662489&#39;); var vizElement = divElement.getElementsByTagName(&#39;object&#39;)[0]; vizElement.style.width=&#39;100%&#39;;vizElement.style.height=(divElement.offsetWidth*0.75)+&#39;px&#39;; var scriptElement = document.createElement(&#39;script&#39;); scriptElement.src = &#39;https://public.tableau.com/javascripts/api/viz_v1.js&#39;; vizElement.parentNode.insertBefore(scriptElement, vizElement); &lt;/script&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In case the graph does not work in your browser, please use this direct link &lt;cite&gt;&lt;a href=&quot;https://public.tableau.com/profile/curious.gnu#!/vizhome/YearlyBillsofMortality1657-1758/Dashboard1&quot;&gt;public.tableau.com&lt;/a&gt;&lt;/cite&gt;. If you have questions or concerns, feel free to write me an &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;email&lt;/a&gt;.&lt;/p&gt;


          </description>
        <pubDate>Wed, 22 Feb 2017 11:00:00 +0100</pubDate>
        <link>https://www.curiousgnu.com/yearly-bills-of-mortality-1657-1758</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/yearly-bills-of-mortality-1657-1758</guid>
      </item>
      
    
      
      <item>
        <title>Visualization: Traffic Collisions in Manhattan</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/manhattan-traffic-collisions/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;Today’s blog post is a visualization of traffic collisions in New York City. The New York Police Departmentpublishes information about motor vehicle collisions on the official &lt;a href=&quot;https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions/h9gi-nx95&quot;&gt;NYC OpenData&lt;/a&gt; platform. The dataset contains nearly one million incidents going back to the year 2012. For the following visualizations, I only looked at accidents that occurred during the year 2016 in Manhattan (n=36,680).&lt;/p&gt;

&lt;p&gt;My first idea was to display all last year’s collisions on an animated map. For this type of task, &lt;a href=&quot;https://carto.com/&quot;&gt;CARTO&lt;/a&gt; (formerly CartoDB) is an excellent freemium service that allows users to create custom maps. Usually, you can upload the OpenData CSV files directly to CARTO; however, for one-fifth of the incidents, no GPS coordinates are provided, and only streets or intersections are given. This lack of information makes a process called geocoding necessary, which transforms postal addresses to geographic coordinates. Due to the relatively large sample size (n=6,301), the free geocoding option of CARTO wasn’t sufficient, so I used the website, &lt;a href=&quot;https://geocod.io/&quot;&gt;Geocodio&lt;/a&gt;, which offers a very affordable pay-as-you-go plan. For this map, I differentiated between collisions that lead to material damage or personal injury.&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;530&quot; frameborder=&quot;0&quot; src=&quot;https://curiousgnu.carto.com/viz/4fba7a94-ec7b-11e6-b3db-0e233c30368f/embed_map&quot; allowfullscreen=&quot;&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Aside from this, we can of course also use the data for other types of plots. The following line plots compare the number of collisions between times of day and months.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/manhattan-traffic-collisions/line_charts.png&quot;&gt;&lt;img src=&quot;/assets/images/manhattan-traffic-collisions/line_charts.png&quot; alt=&quot;Traffic Collitions in 2016 in Mannhattan&quot; style=&quot;max-width: 550px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In addition to these simple visualizations, it could be interesting to check how the weather affects the number of accidents (&lt;a href=&quot;https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets&quot;&gt;climatological data&lt;/a&gt;). If you have questions, feel free to write me an &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;email&lt;/a&gt;.&lt;/p&gt;


          </description>
        <pubDate>Thu, 09 Feb 2017 11:00:00 +0100</pubDate>
        <link>https://www.curiousgnu.com/manhattan-traffic-collitions</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/manhattan-traffic-collitions</guid>
      </item>
      
    
      
      <item>
        <title>Age Differences of Celebrity Couples</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/age-differences-celebrity-couples/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;In one of my &lt;a href=&quot;https://www.curiousgnu.com/imdb-age-gap&quot;&gt;first posts&lt;/a&gt;, I wrote about the age difference of movie couples and showed that the male is, on average, slightly older than his partner. The goal of this article is to find out whether there is a similar trend among real celebrity couples. Even though the age difference in sexual relationships is a frequent subject of proper scientific studies, I thought it would be still interesting to take a closer look at stars’ dating lives.&lt;/p&gt;

&lt;p&gt;As a data source, I scraped the website &lt;a href=&quot;http://www.whosdatedwho.com/&quot;&gt;whosdatedwho.com&lt;/a&gt; (WDW), which collects information about the dating history of celebrities. Since it is a typical gossip site, and also relies on rumors, it must be expected that some of the data is inaccurate or incomplete. Nevertheless, this should be acceptable for our use case. The web-scraping provided data on 53,820 celebrities (dataset: 2016/12/06). To ensure the relevance of the data, I only included relationships which started in the past 50 years in the sample where at least one partner had been among the 5,000 most searched celebrities (WDW Rank). In addition, I removed all relationships from the sample for which the birthdates of both partners were not available. After applying all the criteria, the sample consisted of 6,693 relationships.&lt;/p&gt;

&lt;p&gt;While extracting the data from the HTML source was a straightforward process with the Python library, &lt;a href=&quot;https://www.crummy.com/software/BeautifulSoup/&quot;&gt;Beautiful Soup&lt;/a&gt;, it was surprising that WDW doesn’t provide any information about gender. In cases where the sex couldn’t be derived from the occupation (e.g., actor/actresses), I used &lt;a href=&quot;https://genderize.io/&quot;&gt;genderize.io&lt;/a&gt; to determine the gender from the first name and manually checked unclear cases.&lt;/p&gt;

&lt;p&gt;To visualize the data, we can use a program like &lt;a href=&quot;https://gephi.org/&quot;&gt;Gephi&lt;/a&gt; to plot the relationships as a network. For the following figure, I used Gephi’s ego network filter with a depth setting of 2 to illustrate to whom &lt;em&gt;Alexander Skarsgård&lt;/em&gt;, a Swedish actor, was directly and indirectly connected.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/age-differences-celebrity-couples/net.png&quot;&gt;&lt;img src=&quot;/assets/images/age-differences-celebrity-couples/net.png&quot; alt=&quot;Whosdatedwho.com Network&quot; style=&quot;max-width:500px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This figure indicates that both &lt;em&gt;Alexander Skarsgård&lt;/em&gt; and &lt;em&gt;Marilyn Manson&lt;/em&gt;, an American singer, dated the actress &lt;em&gt;Evan Rachel Wood&lt;/em&gt;. Next, let’s look at the ages of the partners at the start of their (alleged) relationships. The 2D density plot shows the age of the male partner on the x-axis and the age of the female partner on the y-axis. The plot suggests that, in many cases, the man is slightly older than his female partner, a difference that gets bigger with the increasing age of the male. Overall, this age disparity is similar to the age difference among married couples in Western countries (&lt;a href=&quot;https://en.wikipedia.org/wiki/Age_disparity_in_sexual_relationships&quot;&gt;Wikipedia&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/age-differences-celebrity-couples/density.png&quot;&gt;&lt;img src=&quot;/assets/images/age-differences-celebrity-couples/density.png&quot; alt=&quot;Couples - Density Plot&quot; style=&quot;max-width:500px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To visualize the age difference across certain age groups, I used the R package &lt;a href=&quot;https://cran.r-project.org/web/packages/yarrr/index.html&quot;&gt;yarrr&lt;/a&gt; for the next pirate plot. The plot illustrates that the age disparity in new relationships increases significantly when male celebrities get older. While male stars in their &lt;strong&gt;20s&lt;/strong&gt; have female partners  &lt;strong&gt;2.4 years&lt;/strong&gt; older on average, the women in new relationships with &lt;strong&gt;40- to 49-year-olds&lt;/strong&gt; and &lt;strong&gt;over-50s&lt;/strong&gt; are about &lt;strong&gt;6.0&lt;/strong&gt; and &lt;strong&gt;16.5 years younger&lt;/strong&gt;, respectively.&lt;/p&gt;

&lt;style type=&quot;text/css&quot;&gt;
	.tg-ba  {border-collapse:collapse;border-spacing:0;border:none;max-width: 400px;border-top: 2px #333 solid;border-bottom: 2px #333 solid;}
	.tg-ba td{font-size:14px;padding:1px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;background-color:#fff!important;text-align:center}
	.tg-ba th{font-size:14px;font-weight:normal;padding:1px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;background-color:#fff!important;text-align:center;border-bottom: 1px #333 solid;}
	.tg-ba .tg-c9cr{font-style:italic}
	.tg-ba .tg-wxgh{text-decoration:underline;vertical-align:top}
	.tg-ba .tg-yw4l{vertical-align:top}
	.tg-ba .tg-db0a{text-decoration:underline}
&lt;/style&gt;

&lt;center&gt;
	&lt;table class=&quot;tg-ba&quot;&gt;
		&lt;tr&gt;
			&lt;th class=&quot;tg-031e&quot;&gt;&lt;/th&gt;
			&lt;th class=&quot;tg-c9cr&quot; colspan=&quot;2&quot;&gt;age difference&lt;/th&gt;
			&lt;th class=&quot;tg-yw4l&quot;&gt;&lt;/th&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td class=&quot;tg-db0a&quot;&gt;age (male)&lt;/td&gt;
			&lt;td class=&quot;tg-db0a&quot;&gt;mean&lt;/td&gt;
			&lt;td class=&quot;tg-wxgh&quot;&gt;median&lt;/td&gt;
			&lt;td class=&quot;tg-wxgh&quot;&gt;n&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td class=&quot;tg-031e&quot;&gt;20-29&lt;/td&gt;
			&lt;td class=&quot;tg-031e&quot;&gt;-2.39&lt;/td&gt;
			&lt;td class=&quot;tg-yw4l&quot;&gt;-1&lt;/td&gt;
			&lt;td class=&quot;tg-yw4l&quot;&gt;420&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td class=&quot;tg-031e&quot;&gt;30-39&lt;/td&gt;
			&lt;td class=&quot;tg-031e&quot;&gt;0.51&lt;/td&gt;
			&lt;td class=&quot;tg-yw4l&quot;&gt;1&lt;/td&gt;
			&lt;td class=&quot;tg-yw4l&quot;&gt;3,170&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td class=&quot;tg-031e&quot;&gt;40-49&lt;/td&gt;
			&lt;td class=&quot;tg-031e&quot;&gt;5.99&lt;/td&gt;
			&lt;td class=&quot;tg-yw4l&quot;&gt;7&lt;/td&gt;
			&lt;td class=&quot;tg-yw4l&quot;&gt;2,130&lt;/td&gt;
		&lt;/tr&gt;
		&lt;tr&gt;
			&lt;td class=&quot;tg-031e&quot;&gt;50+&lt;/td&gt;
			&lt;td class=&quot;tg-031e&quot;&gt;16.47&lt;/td&gt;
			&lt;td class=&quot;tg-yw4l&quot;&gt;17&lt;/td&gt;
			&lt;td class=&quot;tg-yw4l&quot;&gt;973&lt;/td&gt;
		&lt;/tr&gt;
	&lt;/table&gt;
&lt;/center&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/age-differences-celebrity-couples/pirate.png&quot;&gt;&lt;img src=&quot;/assets/images/age-differences-celebrity-couples/pirate.png&quot; alt=&quot;Couples - Pirate Plot&quot; style=&quot;max-width:500px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A regression analysis can be applied to examine the relationship between female and male age further. To automatically account for non-linearity, I chose &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines&quot;&gt;MARS&lt;/a&gt; (&lt;a href=&quot;https://cran.r-project.org/web/packages/earth/index.html&quot;&gt;earth&lt;/a&gt; R package). The result shows the age difference changes with the increasing age of the male partner. The shaded area is the 90% prediction interval. According to the model, the new partner (female) of a &lt;strong&gt;40-year-old&lt;/strong&gt; male celebrity who started dating is on average &lt;strong&gt;9.8 years younger&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/age-differences-celebrity-couples/regression.png&quot;&gt;&lt;img src=&quot;/assets/images/age-differences-celebrity-couples/regression.png&quot; alt=&quot;Couples - Pirate Regression&quot; style=&quot;max-width:600px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Overall, the data shows that the often criticized age difference among movie couples may not be too far from reality, or at least the reality of some celebrities. If you have questions or concerns, feel free to write me an &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;email&lt;/a&gt;.&lt;/p&gt;


          </description>
        <pubDate>Thu, 26 Jan 2017 13:00:00 +0100</pubDate>
        <link>https://www.curiousgnu.com/age-differences-celebrity-couples</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/age-differences-celebrity-couples</guid>
      </item>
      
    
      
      <item>
        <title>A Darknet Site Currently Offers 42,497 U.S. Credit Cards</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/darknet-credit-cards/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;Last week, I found out that my credit card information was stolen and used by criminals for an expensive shopping spree. This incident inspired me to revisit my &lt;a href=&quot;https://www.curiousgnu.com/cryptomarkets-lsd-sales&quot;&gt;previous post&lt;/a&gt; about drug dealing on the Darknet and research how stolen credit cards are traded there. I was able to gather a dataset of &lt;strong&gt;42,497&lt;/strong&gt; stolen U.S. credit cards which are currently sold on a Darknet site. At the end of this post, you’ll find a link to a site that allows you to see if your card is in this dataset.&lt;/p&gt;

&lt;p&gt;The Darknet, a part of the internet which is only accessible with special software like the &lt;a href=&quot;https://www.torproject.org/projects/torbrowser.html.en&quot;&gt;Tor Browser&lt;/a&gt;, allows users and website owners to stay anonymous. The anonymity not only makes the Darknet an essential tool for people like reporters or activists, but is also used widely by criminals. Nowadays the Darknet hosts multiple &lt;a href=&quot;https://en.wikipedia.org/wiki/Darknet_market&quot;&gt;Cryptomarkets&lt;/a&gt;, which are marketplaces similar to eBay where vendors can sell products like illicit drugs, counterfeit money, or stolen credit cards. I decided to take a close look at the AlphaBay Market because it’s one of the largest marketplaces and relatively easy to scrape.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/darknet-credit-cards/alphabay_screenshot.png&quot;&gt;&lt;img src=&quot;/assets/images/darknet-credit-cards/alphabay_screenshot.png&quot; alt=&quot;AlphaBay - Screenshot&quot; style=&quot;max-width: 600px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The AlphaBay Market has a feature called &lt;em&gt;CC Autoshop&lt;/em&gt; that allows potential buyers to search a database of all the stolen credit cards that are offered by different sellers. For example, it’s possible to only search for cards from a particular city. That’s important for criminals who try to circumvent anti-fraud measures by using stolen cards from the same region or state in which they are located. But it also means that the sellers have to reveal information that we can analyze.&lt;/p&gt;

&lt;p&gt;First, I downloaded the entire &lt;em&gt;CC Autoshop&lt;/em&gt; as HTML files before I used the Python library &lt;em&gt;Beautiful Soup&lt;/em&gt; to extract the information. I got 42,497 U.S. credit cards which were offered on September 1st, 2016 as a result. The total value of is &lt;strong&gt;$324,941&lt;/strong&gt; with an average price of &lt;strong&gt;$7.65 per card&lt;/strong&gt;. Some cards (1025) even include the Social Security number of the owner. Considering that nearly &lt;a href=&quot;http://www.creditcards.com/credit-card-news/credit-card-security-id-theft-fraud-statistics-1276.php&quot;&gt;13 million Americans&lt;/a&gt; are victims of identity fraud each year, this number almost seems insignificant, but it can nevertheless cause millions of dollars in damages. What I find particularly concerning is that anyone can purchase these cards after doing basic internet research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/darknet-credit-cards/map.jpg&quot;&gt;&lt;img src=&quot;/assets/images/darknet-credit-cards/map.jpg&quot; alt=&quot;AlphaBay - Countries&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The map above shows how many cards from each state are offered on the AlphaBay Market. It is suprising that &lt;strong&gt;27%&lt;/strong&gt; (11,338) of them are from Delaware and are sold by only two major sellers (&lt;em&gt;Sasha_Grey&lt;/em&gt; and &lt;em&gt;HotPizza&lt;/em&gt;). Unfortunately, I can only speculate about the reasons for this. Are those just company credit cards or does it suggest a local card breach? The following graph is an overview of all active AlphaBay CC vendors based on the total number of cards they offer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/darknet-credit-cards/treemap.png&quot;&gt;&lt;img src=&quot;/assets/images/darknet-credit-cards/treemap.png&quot; alt=&quot;AlphaBay - Treemap&quot; style=&quot;max-width: 600px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you would like to check whether your credit card is in the dataset, you can either download it as a &lt;a href=&quot;https://www.curiousgnu.com/assets/data/cc_autobuy_public.csv.gz&quot;&gt;CSV file&lt;/a&gt; or use the following site to search the database: &lt;a href=&quot;https://www.curiousgnu.com/assets/tools/cc.html&quot;&gt;www.curiousgnu.com/assets/tools/cc.html&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;how-to-scrape-darknet-sites&quot;&gt;How to scrape Darknet sites?&lt;/h4&gt;

&lt;p&gt;Scraping &lt;a href=&quot;https://www.torproject.org/docs/hidden-services.html.en&quot;&gt;&lt;em&gt;hidden services&lt;/em&gt;&lt;/a&gt; in the Tor network (Darknet sites) is very similar to regular web scraping with the exception that most &lt;em&gt;hidden services&lt;/em&gt; don’t use JSON APIs, meaning that you have to extract the information from an HTML file. My preferred method is first to download all pages as HTML files before using Python and &lt;em&gt;Beautiful Soup&lt;/em&gt; to extract the information I need. The following steps should work on OS X and Linux systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/darknet-credit-cards/alphabay_curl_web.png&quot;&gt;&lt;img src=&quot;/assets/images/darknet-credit-cards/alphabay_curl_web.png&quot; alt=&quot;AlphaBay - CURL&quot; style=&quot;max-width: 500px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Visit the site you want to scrape in the Tor Browser.&lt;/li&gt;
  &lt;li&gt;Open the Web Console (&lt;em&gt;Right Click &amp;gt; Inspect Element&lt;/em&gt;).&lt;/li&gt;
  &lt;li&gt;Go to the &lt;em&gt;Network&lt;/em&gt; Tab.&lt;/li&gt;
  &lt;li&gt;Right click on the main GET request (see Domain column) and select Copy as cURL to copy the curl command to replicate the request.&lt;/li&gt;
  &lt;li&gt;Do not close the Tor browser.&lt;/li&gt;
  &lt;li&gt;Before you can run the curl command in the Terminal, add the following options to the end of it &lt;strong&gt;&lt;em&gt;’–socks5-hostname 127.0.0.1:9150 –output o.html’&lt;/em&gt;&lt;/strong&gt;. It tells curl to use the Tor client and save the output to o.html.&lt;/li&gt;
  &lt;li&gt;In case you want to download multiple pages, you can use the bash &lt;em&gt;for loop&lt;/em&gt; like this 
&lt;strong&gt;&lt;em&gt;‘for i in {1..10}; do curl […] page=${i} –output ${i}.html; done’&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Follow the Beautiful Soup &lt;a href=&quot;https://www.crummy.com/software/BeautifulSoup/bs4/doc/&quot;&gt;documentation&lt;/a&gt; to extract the need information from the saved HTML files.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you have questions or concerns, feel free to write me an &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;email&lt;/a&gt;.&lt;/p&gt;


          </description>
        <pubDate>Tue, 06 Sep 2016 03:00:00 +0200</pubDate>
        <link>https://www.curiousgnu.com/darknet-credit-cards</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/darknet-credit-cards</guid>
      </item>
      
    
      
      <item>
        <title>Almost 80% of Private Day Traders Lose Money</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/day-trading/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;A few months ago, I wrote a &lt;a href=&quot;http://www.curiousgnu.com/penny-auctions&quot;&gt;blog post&lt;/a&gt; about how penny auction sites make you money. As a reaction, some readers sent me links to day trading brokers that promise easy returns. These brokers allow private investors to hold stocks or currencies positions for a short time which makes it possible to speculate on small price changes. Many day traders use the margin and leverage to increase the size of their positions by lending money from their brokers.&lt;/p&gt;

&lt;p&gt;For example, a 1:10 leverage increases the profits by the factor 10 but also the potential losses. Strictly speaking, only trading within a day is called day trading. For this post, I’ll use a broader definition which also includes leveraged short-term trades where the positions are held for multiple days.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/day-trading/banner.png&quot; alt=&quot;Forex Banner&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In contrast to many penny auctions sites, these brokers are mostly legitimate and are regulated companies. However, this fact doesn’t make this kind of trading any less risky. My goal is to find out if the average investor profits from day trading.&lt;/p&gt;

&lt;h4 id=&quot;data&quot;&gt;Data&lt;/h4&gt;
&lt;p&gt;The data source for this post is &lt;a href=&quot;https://en.wikipedia.org/wiki/EToro&quot;&gt;&lt;em&gt;eToro&lt;/em&gt;&lt;/a&gt;, a brokerage company that offers a feature called &lt;a href=&quot;https://www.etoro.com/en/social-trading/&quot;&gt;Social Trading&lt;/a&gt;, which is social network for traders. It is enabled by default and allows users to view and copy other users’ trades. Therefore, everyone’s trading performance is publicly available who have not disabled Social Trading.&lt;/p&gt;

&lt;p&gt;On the 1st of August, 2016, I downloaded the publicly available data through their ranking API. I selected all users who were active during the past twelve months, traded with real money, and had at least three trades. The results consist of &lt;strong&gt;83.3k traders&lt;/strong&gt; who fulfill these conditions. If you’re interested in how you can access the (undocumented) API, I recommend you to open Chrome’s &lt;em&gt;DevTools&lt;/em&gt;, while you’re on eToro’s Discover People page.&lt;/p&gt;

&lt;h4 id=&quot;results&quot;&gt;Results&lt;/h4&gt;
&lt;p&gt;The following histogram shows the average gains of each trader over the past twelve months. In the end, &lt;strong&gt;79.5%&lt;/strong&gt; of them lost real money. The median 12-month returns were &lt;strong&gt;-36.3%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/day-trading/histogram.png&quot;&gt;&lt;img src=&quot;/assets/images/day-trading/histogram.png&quot; alt=&quot;eToro 12M Gains&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Besides the investing performance, the data also reveals from which countries the traders are coming. With a share of &lt;strong&gt;15.7%&lt;/strong&gt;, the UK leads the list of most common countries followed by Germany with a share of &lt;strong&gt;11.3%&lt;/strong&gt;. The US doesn’t appear in this list because &lt;em&gt;eToro&lt;/em&gt; isn’t available in the US market, presumably due to stricter regulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/day-trading/barchart.png&quot;&gt;&lt;img src=&quot;/assets/images/day-trading/barchart.png&quot; alt=&quot;eToro Customers by Country&quot; style=&quot;max-width: 720px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;The results show that day-trading is a highly risky investment on which most traders end up losing money. I wouldn’t go so far as to say that it’s impossible to make a profit in the long-term but apparently there is no easy method (e.g. technical analysis or social trading) to do it. I would be very careful, if someone promises easy money by trading based on simple patterns or trading signals.&lt;/p&gt;


          </description>
        <pubDate>Wed, 17 Aug 2016 03:00:00 +0200</pubDate>
        <link>https://www.curiousgnu.com/day-trading</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/day-trading</guid>
      </item>
      
    
      
      <item>
        <title>These Are The Most Dangerous PokeStops in NYC</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/pokemon-go/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;Pokemon GO quickly became one of the most popular mobile games. In cities all around the world, you can see people searching for Pokemon and battling other players in Poke Gyms. While exploring this new augmented reality, it’s easy to forget about the dangerous of the real word. On a daily basis, news sites report on Pokemon Go related incidents like &lt;a href=&quot;https://www.washingtonpost.com/news/morning-mix/wp/2016/08/02/arizona-couple-accused-of-abandoning-son-for-pokemon-go/&quot;&gt;child abandonment&lt;/a&gt;, &lt;a href=&quot;https://www.theguardian.com/australia-news/2016/jul/29/pokemon-go-player-crashes-car-into-school-while-playing-game&quot;&gt;reckless driving&lt;/a&gt;, or &lt;a href=&quot;http://reut.rs/2aJ8W9n&quot;&gt;trespassing&lt;/a&gt;. Earlier this month New York Governor Andrew M. Cuomo even &lt;a href=&quot;http://www.npr.org/sections/alltechconsidered/2016/08/02/488435018/new-york-bans-registered-sex-offenders-from-pok-mon-go&quot;&gt;banned sex offenders&lt;/a&gt; from playing the game.&lt;/p&gt;

&lt;p&gt;These incidents gave me the idea for this article about &lt;a href=&quot;https://support.pokemongo.nianticlabs.com/hc/en-us/articles/221957688&quot;&gt;PokeStops&lt;/a&gt; in potentially unsafe areas where caution is advised. My goal was to analyze public data to identify PokeStops in New York City which are close to crime scenes and registered sex offenders.&lt;/p&gt;

&lt;h4 id=&quot;pokestops-near-crime-scenes&quot;&gt;PokeStops Near Crime Scenes&lt;/h4&gt;

&lt;p&gt;First, I used &lt;a href=&quot;http://www.pokemongomap.info/&quot;&gt;PokemonGOMap.info&lt;/a&gt; to get the locations of the 24 thousand PokeStop in NYC and download all reported felonies of 2015 (103k) from the &lt;a href=&quot;https://data.cityofnewyork.us/Public-Safety/NYPD-7-Major-Felony-Incidents/hyij-8hr7&quot;&gt;NYC OpenData portal&lt;/a&gt;. For this analysis, I exclude the offenses &lt;em&gt;burglary&lt;/em&gt; (15k) and &lt;em&gt;grand larceny&lt;/em&gt; (49k) because they’re less a potential threat to players in the area. The following map shows all PokeStops as blue dots whereas incidents of &lt;em&gt;felony assault &amp;amp; robbery&lt;/em&gt; (37k) are represented by green dots and &lt;em&gt;murder &amp;amp; rape&lt;/em&gt; (1.5k) by red dots.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/pokemon-go/incidents.jpg&quot;&gt;&lt;img src=&quot;/assets/images/pokemon-go/incidents.jpg&quot; alt=&quot;Incidents&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Next, I loaded the raw data into &lt;a href=&quot;https://cran.r-project.org/&quot;&gt;R&lt;/a&gt;* to count all crimes that occurred within 150m (492ft) of each PokeStop. I had to choose this rather large area because the public data doesn’t show the exact location of the incidents due to privacy reason. Instead, it only provides the midpoint of the street segment on which they happened.&lt;/p&gt;

&lt;p&gt;The map below shows the top ten PokeStop in which proximity most major felony incidents occurred. For this map, the offenses murder and rape are weighted by the factor five and only the top PokeStops of an NTA are included to reduce regional clusters. You can find the total number of murders &amp;amp; rapes in the areas of the PokeStops in the red boxes and total number of felony assaults &amp;amp; robberies in the yellow boxes right next to them. The average number of incidents of all NYC PokeStops stands at &lt;strong&gt;0.15&lt;/strong&gt; for murder or rape and &lt;strong&gt;4.00&lt;/strong&gt; for &lt;em&gt;felony assault&lt;/em&gt; or &lt;em&gt;robbery&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/pokemon-go/top_pokespots.png&quot;&gt;&lt;img src=&quot;/assets/images/pokemon-go/top_pokespots.png&quot; alt=&quot;Most Dangerous PokeStops&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;pokestops-close-to-sex-offenders&quot;&gt;PokeStops Close to Sex Offenders&lt;/h4&gt;

&lt;p&gt;Another question I researched was how many registered sex offenders live close to PokeStops. I downloaded their addresses from the website &lt;a href=&quot;http://familywatchdog.us/&quot;&gt;familywatchdog.us&lt;/a&gt;. For the analysis I only selected people who were convicted for offenses against children and/or rape. The following map shows all PokeStops with the color of the dots indicating how many registered sex offenders live in a 150m (492ft) radius.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/pokemon-go/sex_offender.jpg&quot;&gt;&lt;img src=&quot;/assets/images/pokemon-go/sex_offender.jpg&quot; alt=&quot;Sex Offender&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The numbers show that &lt;strong&gt;11.4%&lt;/strong&gt; of all PokeStops in NYC have at least one sex offender living nearby. The next table lists the top ten PokeStops by the total number of offenders living within 150m.&lt;/p&gt;

&lt;table style=&quot;font-size:14px&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;#&lt;/th&gt;
      &lt;th&gt;PokeStop&lt;/th&gt;
      &lt;th&gt;Address&lt;/th&gt;
      &lt;th&gt;Sex Offender (within 150m)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Iglesia Church of Salvation&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://maps.google.com/?q=3110%20Church%20Ave%2C%20Brooklyn%2C%20NY%2011226&quot; target=&quot;_blank&quot;&gt;3110 Church Ave, Brooklyn, NY 11226&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Center For Figurative Painting&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://maps.google.com/?q=261%20W%2035th%20St%2C%20New%20York%2C%20NY%2010001&quot; target=&quot;_blank&quot;&gt;261 W 35th St, New York, NY 10001&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Power Shield Art&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://maps.google.com/?q=252%20W%2037th%20St%2C%20New%20York%2C%20NY%2010018&quot; target=&quot;_blank&quot;&gt;252 W 37th St, New York, NY 10018&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Garment Wear Arcade&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://maps.google.com/?q=306%20W%2037th%20St%2C%20New%20York%2C%20NY%2010018&quot; target=&quot;_blank&quot;&gt;306 W 37th St, New York, NY 10018&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Houndstooth Pub&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://maps.google.com/?q=266%20W%2037th%20St%2C%20New%20York%2C%20NY%2010018&quot; target=&quot;_blank&quot;&gt;266 W 37th St, New York, NY 10018&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;Chill Cat&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://maps.google.com/?q=247-265%20W%2037th%20St%2C%20New%20York%2C%20NY%2010018&quot; target=&quot;_blank&quot;&gt;247-265 W 37th St, New York, NY 10018&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Church&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://maps.google.com/?q=1800%20Bedford%20Ave%2C%20Brooklyn%2C%20NY%2011225&quot; target=&quot;_blank&quot;&gt;1800 Bedford Ave, Brooklyn, NY 11225&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;The Theatre Building&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://maps.google.com/?q=312%20W%2036th%20St%2C%20New%20York%2C%20NY%2010018&quot; target=&quot;_blank&quot;&gt;312 W 36th St, New York, NY 10018&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Memorial of Electrical Diagrams&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://maps.google.com/?q=555%208th%20Ave%2C%20New%20York%2C%20NY%2010018&quot; target=&quot;_blank&quot;&gt;555 8th Ave, New York, NY 10018&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Chanin Commemorative Plaque&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://maps.google.com/?q=41-99%20E%2041st%20St%2C%20New%20York%2C%20NY%2010017&quot; target=&quot;_blank&quot;&gt;41-99 E 41st St, New York, NY 10017&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If you have questions or concerns, feel free to write me an &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;email&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;small&gt;*R packages used: &lt;a href=&quot;https://cran.r-project.org/web/packages/ggmap/index.html&quot;&gt;ggmap&lt;/a&gt;, &lt;a href=&quot;https://cran.r-project.org/web/packages/GISTools/index.html&quot;&gt;GISTools&lt;/a&gt;, &lt;a href=&quot;https://cran.r-project.org/web/packages/rgeos/index.html&quot;&gt;rgeos&lt;/a&gt;, &lt;a href=&quot;https://cran.r-project.org/web/packages/maptools/index.html&quot;&gt;maptools&lt;/a&gt; | Photo: “back at work” by &lt;a href=&quot;https://flic.kr/p/9ze8bf&quot;&gt;Michael Cory&lt;/a&gt; is licensed under CC BY-NC 2.0&lt;/small&gt;&lt;/p&gt;

          </description>
        <pubDate>Tue, 09 Aug 2016 03:00:00 +0200</pubDate>
        <link>https://www.curiousgnu.com/pokemon-go</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/pokemon-go</guid>
      </item>
      
    
      
      <item>
        <title>Conan is The Dirtiest Late-Night Show on YouTube</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/late-night-shows/cover.png&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;I had the idea for this blog post while I was watching some interviews on YouTube. The videos of the &lt;a href=&quot;https://www.youtube.com/user/teamcoco&quot;&gt;&lt;em&gt;Conan&lt;/em&gt;&lt;/a&gt; show stood out to me because many of them seem to be focused on sexual topics. To me, it looks like they were following the simple “sex sells” approach. Not that there’s something inherently wrong with this, it just appears that &lt;em&gt;Conan&lt;/em&gt; uses it much more than other late-night show channels.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/late-night-shows/screenshot.png&quot;&gt;&lt;img src=&quot;/assets/images/late-night-shows/screenshot.png&quot; alt=&quot;Conan Search Results&quot; style=&quot;max-width: 600px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This brought me to my main question. Are &lt;em&gt;Conan&lt;/em&gt; videos more focused on sexual content than the ones of other late-night shows? I decided to compare its YouTube channel to the official channels of &lt;a href=&quot;https://www.youtube.com/user/JimmyKimmelLive&quot;&gt;&lt;em&gt;Jimmy Kimmel Live!&lt;/em&gt;&lt;/a&gt;,  &lt;a href=&quot;https://www.youtube.com/user/latenight&quot;&gt;&lt;em&gt;The Tonight Show Starring Jimmy Fallon&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/channel/UCMtFAi84ehTSYSE9XoHefig&quot;&gt;&lt;em&gt;The Late Show with Stephen Colbert&lt;/em&gt;&lt;/a&gt;, and &lt;a href=&quot;https://www.youtube.com/user/TheLateLateShow&quot;&gt;&lt;em&gt;The Late Late Show with James Corden&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The public &lt;a href=&quot;https://developers.google.com/youtube/v3/docs/videos/list&quot;&gt;YouTube API&lt;/a&gt; allowed me to download the information for all available 12,237 videos. To find out whether a video contains sexual content or not, I compared the video’s title and description against a word list (see below). If the title or description contains at least one of the words, the video will be rated as “contains sexual content”. On top of that, I also checked if the video titles contain names of persons to group the videos into three categories: female, male, and neutral. For example, interviews with actresses fall into the female category, whereas, monologs fall into the neutral category.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/late-night-shows/barplot.png&quot;&gt;&lt;img src=&quot;/assets/images/late-night-shows/barplot.png&quot; alt=&quot;Late Night Show - Bar Plot&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The graph above shows that &lt;strong&gt;17%&lt;/strong&gt; of &lt;em&gt;Conan&lt;/em&gt; videos in the female category contain sexual content which is &lt;strong&gt;11%&lt;/strong&gt; more than the &lt;em&gt;Late Show with Stephen Colbert&lt;/em&gt;, the second place. We also can see that the share of &lt;em&gt;Conan&lt;/em&gt; videos containing sexual content is twice as large in the female category than in the male category.  These numbers confirm the hypothesis that the &lt;em&gt;Conan&lt;/em&gt; YouTube channel focuses much more on sexual content than other late-night shows. &lt;em&gt;The Tonight Show Starring Jimmy Fallon appears&lt;/em&gt; to be the YouTube channel with the cleanest video titles and descriptions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt; The results left me wondering if suggestive titles help the channels to gain views. I created the following plot with the &lt;a href=&quot;https://cran.cnr.berkeley.edu/web/packages/beanplot/&quot;&gt;beanplot&lt;/a&gt; R package which shows us that only &lt;em&gt;Conan&lt;/em&gt; seems to benefit from sexual video titles or descriptions. If you’re interested in the beanplot, you can find a detailed explanation &lt;a href=&quot;https://cran.cnr.berkeley.edu/web/packages/beanplot/vignettes/beanplot.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/late-night-shows/beanplot.png&quot;&gt;&lt;img src=&quot;/assets/images/late-night-shows/beanplot.png&quot; alt=&quot;Late Night Show - Bean Plot&quot; style=&quot;max-width: 680px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Word List: boob*, dating*, hooker*, kiss*, love scene*, naked*, naughty*, nude*, nudity*, orgasm*, panties*, penis*, porn*, prostitute*, sex*, slut*, strip*, topless*, whore*
&lt;/code&gt;&lt;/pre&gt;

          </description>
        <pubDate>Wed, 13 Jul 2016 03:00:00 +0200</pubDate>
        <link>https://www.curiousgnu.com/late-night-shows</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/late-night-shows</guid>
      </item>
      
    
      
      <item>
        <title>Chicago pays female employees only 80% of what it pays male employees</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/pay-gap/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;While I was browsing through the City of &lt;a href=&quot;https://data.cityofchicago.org/&quot;&gt;Chicago’s Data Catalog&lt;/a&gt;, I came across a &lt;a href=&quot;https://data.cityofchicago.org/Administration-Finance/Current-Employee-Names-Salaries-and-Position-Title/xzkq-xp2w&quot;&gt;dataset&lt;/a&gt; of the city’s 32,000 employees which included their full names, position titles, and annual salaries. I thought that it was a great opportunity to find out whether the gender pay gap was a problem there also. The gender pay gap is the average difference between men’s and women’s earnings, which in the US is somewhere around &lt;a href=&quot;http://www.iwpr.org/initiatives/pay-equity-and-discrimination&quot;&gt;-21%&lt;/a&gt; for women. It is an important number many politicians and activists use as proof for gender inequality.&lt;/p&gt;

&lt;p&gt;Before I could compare the average salaries of female and male city employees, I needed to identify their gender – a piece of information which was not included in the official dataset. To do this, I used the R-Package &lt;a href=&quot;https://cran.r-project.org/web/packages/gender/index.html&quot;&gt;&lt;em&gt;gender&lt;/em&gt;&lt;/a&gt; to predict the gender of a person based on his or her first name. Of course, this method isn’t 100% accurate, but because of the high number of employees, this potential inaccuracy shouldn’t be a problem. After that, I was able to compare the average annual salaries of male and female city employees. It turns out that the City of Chicago isn’t any better than the rest of the nation. It pays its female employees on average, only 80% of what their male colleagues make – which is very close to the national average of 79%.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/pay-gap/barchart.png&quot;&gt;&lt;img src=&quot;/assets/images/pay-gap/barchart.png&quot; alt=&quot;Pay Gap Barchart&quot; style=&quot;max-width: 500px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you think now that there are many other factors besides gender, that determine a person’s salary, and that chart above is completely useless, you are right. It’s obviously not good enough to compare only the average earnings of both genders if they do different kinds of jobs. The criticism of how the gender pay gap is used in political discussions isn’t something new, and it has been proven many times that the gender pay gap isn’t a sufficient proof for gender inequality.&lt;/p&gt;

&lt;p&gt;I think one of the problems with the arguments against the gender pay gap is that they often rely on statistical tests. Don’t get me wrong, these tests are the only scientifically correct way to do it; but unfortunately, many people stop listening to you as soon as you start mentioning t-tests and confidence levels. The reason why I find the Chicago dataset so interesting is that it contains the salaries of each employee, which allows us to use it as a real-world example to illustrate the problems with the gender pay gap argument. To do this, I propose a simple scatter plot to display the average male and female salaries per position title.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/pay-gap/sketch.png&quot;&gt;&lt;img src=&quot;/assets/images/pay-gap/sketch.png&quot; alt=&quot;Pay Gap Sketch&quot; style=&quot;max-width: 300px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So, each dot represents one job position, like police officer or police sergeant. If a dot (1) is below the 45-degree line, the average salary of men is higher than the average salary of women holding the same position. If a dot (2) is above the 45-degree line, it’s the other way around. In case the average salaries of both genders are equal, the dot (3) sits directly on the line. Based on this idea, I generated the following plot:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/pay-gap/scatter_plot.png&quot;&gt;&lt;img src=&quot;/assets/images/pay-gap/scatter_plot.png&quot; alt=&quot;Pay Gap Scatter Plot&quot; style=&quot;max-width: 600px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This plot clearly shows that women are not systematically paid 20% less for doing the same job as the first bar chart might have suggested.  The main reason for the difference is that women are doing different jobs than men do. Therefore, the gender pay gap shouldn’t be used as an argument for the existence of gender inequality but gender differences. I’m not saying that gender discrimination doesn’t exist in the workplace, it’s just that the gender pay gap doesn’t support the claim that women are paid 20% less for doing the same job. Therefore, a more honest way to use this statistic would be in a discussion about how both gender and personal choices affect careers.&lt;/p&gt;

&lt;p&gt;In conclusion, it’s true that the gender pay gap exists, and that on average, women make less money than men. However, the claims that it proves gender inequality are false because women are simply doing different kinds of jobs.&lt;/p&gt;

&lt;p&gt;If you have questions or concerns, feel free to write me an  &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;email&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Photo: “Chicago” by &lt;a href=&quot;https://flic.kr/p/62vAsR&quot;&gt;Tony Webster&lt;/a&gt; is licensed under CC BY 2.0&lt;/small&gt;&lt;/p&gt;

          </description>
        <pubDate>Tue, 05 Jul 2016 03:00:00 +0200</pubDate>
        <link>https://www.curiousgnu.com/pay-gap</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/pay-gap</guid>
      </item>
      
    
      
      <item>
        <title>Using Amazon&#39;s X-Ray to Visualize Characters&#39; Screen Time</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/movie-character-screen-time/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;Today’s blog post is once again about the visualization of movie data. As I already experimented with the IMDb dataset to &lt;a href=&quot;http://www.curiousgnu.com/imdb-age-gap&quot;&gt;compare the average age&lt;/a&gt; of actors and actresses, I wanted to try something a bit different. One thing that I have always found cool is the visualization of movie plots (e.g. &lt;a href=&quot;https://xkcd.com/657/&quot;&gt;xkcd&lt;/a&gt;). The reason why I never attempted to do something like this myself was that I had no idea from where I could get the required data. Of course, there is always the possibility to generate the data manually, but that is usually a tedious task that I try to avoid. Fortunately, I found a much more convenient data source, while I was watching a movie on the &lt;a href=&quot;https://www.amazon.com/video&quot;&gt;Amazon Video&lt;/a&gt; app.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/movie-character-screen-time/xray_screenshot.jpg&quot;&gt;&lt;img src=&quot;/assets/images/movie-character-screen-time/xray_screenshot.jpg&quot; alt=&quot;X-Ray Screenshot&quot; style=&quot;max-width: 480px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Its X-Ray feature shows you relevant IMDb information based on which actor is currently in the scene. The app does that based on a single text file which contains the information for when a character appears in a scene. At the end of the post, I will describe how you can extract the file yourself. First, I downloaded the X-Ray file for the latest &lt;a href=&quot;https://www.amazon.com/dp/B019G7X9E6&quot;&gt;Star Wars movie&lt;/a&gt;. Based on this data we can compare the characters by their screen time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/movie-character-screen-time/barplot.png&quot;&gt;&lt;img src=&quot;/assets/images/movie-character-screen-time/barplot.png&quot; alt=&quot;Characters Screen Time by Minutes&quot; style=&quot;max-width: 647px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I noticed that the numbers are not always 100% accurate because some characters are only visible in parts of a scene. However, it should not be a major problem for which we are using them in this post. Next, I used the &lt;a href=&quot;https://cran.r-project.org/web/packages/ggplot2/&quot;&gt;&lt;em&gt;ggplot2&lt;/em&gt;&lt;/a&gt; package in R to plot the following Gantt chart:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/movie-character-screen-time/tp_starwars.png&quot;&gt;&lt;img src=&quot;/assets/images/movie-character-screen-time/tp_starwars.png&quot; alt=&quot;Star Wars The Force Awakens&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We can use the X-Ray data, not only to identify in which scene a character appears but also with whom else. To visualize this information, I used &lt;a href=&quot;https://gephi.org/&quot;&gt;&lt;em&gt;Gephi&lt;/em&gt;&lt;/a&gt;, an open source tool to plot networks. My assumption is that the longer characters appear on-screen together, the closer their relationship is. The circle size is based on their total screen time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/movie-character-screen-time/network.png&quot;&gt;&lt;img src=&quot;/assets/images/movie-character-screen-time/network.png&quot; alt=&quot;Star Wars - Network Plot&quot; style=&quot;max-width: 512px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I hope these examples show what you can do with Amazon X-Ray data relatively quickly. The best thing of this approach is that it only requires a minimum manual work. So, here are Gantt charts for three other movies I enjoy:
&lt;a href=&quot;/assets/images/movie-character-screen-time/tp_big_lebowski.png&quot;&gt;&lt;img src=&quot;/assets/images/movie-character-screen-time/tp_big_lebowski.png&quot; alt=&quot;The Big Lebowski&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/movie-character-screen-time/tp_pulp_fiction.png&quot;&gt;&lt;img src=&quot;/assets/images/movie-character-screen-time/tp_pulp_fiction.png&quot; alt=&quot;Pulp Fiction&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/movie-character-screen-time/tp_john_wick.png&quot;&gt;&lt;img src=&quot;/assets/images/movie-character-screen-time/tp_john_wick.png&quot; alt=&quot;John Wick&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;how-to-get-x-ray-data&quot;&gt;How to Get X-Ray Data?&lt;/h4&gt;

&lt;p&gt;The X-Ray feature is based on an unencrypted JSON file which can be downloaded with the Chrome browser. Unfortunately, those files are not publicly available (signed CloudFront URL), meaning that you have to start streaming the movie before you can download the file. This also means that you are limited to the content included in your Prime subscription, or you need to rent/buy the movies in which you are interested. Nevertheless, I think it is still an interesting source, especially when you consider the alternatives.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Start Developer Tools: Menu &amp;gt; Tools &amp;gt; Developer Tools&lt;/li&gt;
  &lt;li&gt;Start streaming a video &amp;amp; close the player after a few seconds&lt;/li&gt;
  &lt;li&gt;Select the following Developer Tools settings:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/movie-character-screen-time/devtools.png&quot;&gt;&lt;img src=&quot;/assets/images/movie-character-screen-time/devtools.png&quot; alt=&quot;DevTools Settings&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Click on the gray record button to capture the network traffic&lt;/li&gt;
  &lt;li&gt;Start streaming the video again&lt;/li&gt;
  &lt;li&gt;Now the following file should appear: &lt;em&gt;data.json?Expires=&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Right-click on the file &amp;gt; Open Link in New Tab &amp;gt; Save Page As…&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then you can use in R the &lt;a href=&quot;https://cran.r-project.org/web/packages/jsonlite/index.html&quot;&gt;&lt;em&gt;jsonlite&lt;/em&gt;&lt;/a&gt; package to load the JSON file and then do, for example, something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(jsonlite)
library(ggplot2)

data &amp;lt;- fromJSON(&quot;starwars.json&quot;, flatten = TRUE)
e &amp;lt;- data$resource$events
e$start &amp;lt;- as.numeric(as.data.frame(e$when)[1,])
e$end &amp;lt;- as.numeric(as.data.frame(e$when)[2,])
ggplot(e, aes(colour=e$character)) + 
  geom_segment(aes(x=e$start, xend=e$end, y=e$character, yend=e$character), size=5)
&lt;/code&gt;&lt;/pre&gt;

          </description>
        <pubDate>Wed, 22 Jun 2016 03:00:00 +0200</pubDate>
        <link>https://www.curiousgnu.com/movie-character-screen-time</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/movie-character-screen-time</guid>
      </item>
      
    
      
      <item>
        <title>Which illicit drugs do Chicagoans take?</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/chicago-drugs/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;In one of my &lt;a href=&quot;http://www.curiousgnu.com/cryptomarkets-lsd-sales&quot;&gt;previous blog articles&lt;/a&gt;, I wrote about how drug dealers use the darknet to sell their products. For this post, I will use police reports to show you more about drug possession in the real world. I chose the city of Chicago because they make all reported incidents of crime available through their &lt;a href=&quot;https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2&quot;&gt;open data platform&lt;/a&gt; which is the basis for the following analysis.&lt;/p&gt;

&lt;h5 id=&quot;what-are-the-most-common-drugs&quot;&gt;What are the most common drugs?&lt;/h5&gt;

&lt;p&gt;First, I took all reported incidents of drug possession since 2001 and checked how the numbers have changed over the past 15 years. The following graph shows the number of reported incidents by year and substance. The number of incidents of &lt;em&gt;cannabis&lt;/em&gt; possessions peaked at more than 23k in 2010 and then decreased by nearly 55%, landing at 11k in 2015. Reported incidents related to the possession of &lt;em&gt;crack cocaine&lt;/em&gt; continuously declined over the years, making heroin the second most common drug since 2010. It should be noted that these numbers only refer to the reported incidents of drug possession and not the actual drug consumption.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/chicago-drugs/possession_15_hd.png&quot;&gt;&lt;img src=&quot;/assets/images/chicago-drugs/possession_01_15.gif&quot; alt=&quot;Drug Possession 2001-2015&quot; style=&quot;max-width: 100%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The map on the right shows hot spots in Chicago where many of the incidents took place. This heat map is based on the density of the incidents. In the recent years, we can see a higher concentration of the reports on the West Side.&lt;/p&gt;

&lt;h5 id=&quot;do-drug-preferences-differ-between-areas&quot;&gt;Do drug preferences differ between areas?&lt;/h5&gt;

&lt;p&gt;Next, I wanted to find out if some drugs are more popular in certain areas than the others. To do this, I created three new heat maps for the possession of &lt;em&gt;cocaine&lt;/em&gt;, &lt;em&gt;heroin&lt;/em&gt;, and &lt;em&gt;cannabis&lt;/em&gt;. As we can see below, there seems to be a difference between the three substances:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/chicago-drugs/map_grid.jpg&quot;&gt;&lt;img src=&quot;/assets/images/chicago-drugs/map_grid.jpg&quot; alt=&quot;Possession by Community Areas&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To find an explanation for the local differences, I downloaded &lt;a href=&quot;https://datahub.cmap.illinois.gov/dataset/community-data-snapshots-raw-data-2015/resource/8c4e096e-c90c-4bef-9cf1-9028d094296e&quot;&gt;census data&lt;/a&gt; on a community area level. We can use this data to explore how the community areas differ from each other in factors like education, income, race, or unemployment. If we plot this data in a map format and compare them to the previous heat maps, it shows that there could be a connection between them and the possession of certain drugs.&lt;/p&gt;

&lt;p&gt;Judging by the looks of graphs is of course, not an appropriate method, which is why I also used a spatial regression to test how education, income, race, and unemployment relates to the reported possession of cocaine, heroin, and cannabis. The depended variables are the percentage of reported incidents based on the total population of the community area in which they occurred.&lt;/p&gt;

&lt;p&gt;The results below show that reported cannabis possession is lower in community areas with a higher median income and education. For heroin, we see similar results with the exception that median income is not a significant factor but median age is. For cocaine, the only two significant variables are the relative shares of the Black and Hispanic population. Apparently more incidents of cocaine possession happen in areas with bigger Black and Hispanic populations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/chicago-drugs/regression_table.png&quot;&gt;&lt;img src=&quot;/assets/images/chicago-drugs/regression_table.png&quot; alt=&quot;Regression Results&quot; style=&quot;max-width: 640px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you have any suggestion or tips for future articles, please feel free to contact me by &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;email&lt;/a&gt; or &lt;a href=&quot;https://www.reddit.com/user/CuriousGnu/&quot;&gt;Reddit message&lt;/a&gt;.&lt;/p&gt;

          </description>
        <pubDate>Fri, 27 May 2016 03:00:00 +0200</pubDate>
        <link>https://www.curiousgnu.com/chicago-drugs</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/chicago-drugs</guid>
      </item>
      
    
      
      <item>
        <title>78% of Reddit Threads With 1,000+ Comments Mention Nazis</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/rd-godwin/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;Let me start this post by noting that I will not attempt to test &lt;a href=&quot;https://en.wikipedia.org/wiki/Godwin%27s_law&quot;&gt;Godwin’s Law&lt;/a&gt;, which states that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;As an online discussion grows longer, the probability of a comparison involving Nazis or Hitler approaches 1.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this post, I’ll only try to find out how many Reddit comments mention &lt;em&gt;Nazis&lt;/em&gt; or &lt;em&gt;Hitler&lt;/em&gt; and ignore the context in which they are made. The data source for this analysis is the &lt;a href=&quot;https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2016_03&quot;&gt;Reddit dataset&lt;/a&gt; which is publicly available on Google BigQuery. The following graph is based on &lt;strong&gt;4.6 million&lt;/strong&gt; comments and shows the share of comments mentioning &lt;em&gt;Nazis&lt;/em&gt; or &lt;em&gt;Hitler&lt;/em&gt; by subreddit.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/rd-godwin/subrd.png&quot;&gt;&lt;img src=&quot;/assets/images/rd-godwin/subrd.png&quot; alt=&quot;Hitler Comments by Subreddits&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Then I excluded history subreddits and looked at the probability that a Reddit thread mentions &lt;em&gt;Nazis&lt;/em&gt; or &lt;em&gt;Hitler&lt;/em&gt; at least once. Unsuprisigly, the probability of a Nazi refrence increases as the threads get bigger. Nevertheless, I didn’t expect that the probability would be &lt;strong&gt;over 70%&lt;/strong&gt; for a thread with more than 1,000 comments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/rd-godwin/pbrd.png&quot;&gt;&lt;img src=&quot;/assets/images/rd-godwin/pbrd.png&quot; alt=&quot;Hitler References&quot; style=&quot;max-width:780px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The next step would be to implement sophisticated text mining techniques to identify comments which use Nazi analogies in a way as described by Godwin. Unfortunately due to time constraints and the complexity of this problem, I was not able to try for this blog post.&lt;/p&gt;

          </description>
        <pubDate>Wed, 04 May 2016 03:00:00 +0200</pubDate>
        <link>https://www.curiousgnu.com/reddit-godwin</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/reddit-godwin</guid>
      </item>
      
    
      
      <item>
        <title>How The World Sees Hillary Clinton &amp; Donald Trump</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/trump-clinton/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;For this week’s blog post, I will try to find out how the international news media writes about Hillary Clinton and Donald Trump; the presidential front-runners of both parties. The plan is to check on several international news sources to derive positive and negative things regarding Hillary Clinton or Donald Trump. Doing this will give us an idea of how the news media in other countries, writes and talks about the two candidates.&lt;/p&gt;

&lt;p&gt;Fortunately, most of the hard work was already done by the &lt;a href=&quot;http://gdeltproject.org/&quot;&gt;GDELT Project&lt;/a&gt;, which monitors news sites from all around the world and makes its work freely available for everyone. They even automatically determine how positive or negative news articles are using sentiment analysis. Based on the &lt;a href=&quot;http://gdeltproject.org/data.html&quot;&gt;GDELT dataset&lt;/a&gt;, I created a map for each candidate which shows how the average tone of the texts compares to American news (Clinton: -1.15; Trump: -1.40). The results are based on a total of over &lt;strong&gt;550,000 articles&lt;/strong&gt; published after July 2015 of which 65.3% mentioned &lt;em&gt;Donald Trump&lt;/em&gt; at least twice, and 46.1% mentioned &lt;em&gt;Hillary Clinton&lt;/em&gt; at least twice.&lt;/p&gt;

&lt;div class=&quot;emb-iframe&quot;&gt;
  &lt;iframe height=&quot;612&quot; width=&quot;980&quot; frameborder=&quot;0&quot; src=&quot;https://curiousgnu.cartodb.com/viz/5497e1dc-0991-11e6-87dd-0e5db1731f59/embed_map&quot; allowfullscreen=&quot;&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;div class=&quot;emb-iframe&quot;&gt;
  &lt;iframe height=&quot;612&quot; width=&quot;980&quot; frameborder=&quot;0&quot; src=&quot;https://curiousgnu.cartodb.com/viz/676de180-0991-11e6-9940-0e674067d321/embed_map&quot; allowfullscreen=&quot;&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; oallowfullscreen=&quot;&quot; msallowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Compared to the Republican front-runner Donald Trump, international journalists seem to view Hillary Clinton much more positively. Looking at the maps above, we can see that news articles from countries like &lt;em&gt;Mexico&lt;/em&gt;, &lt;em&gt;India&lt;/em&gt;, or &lt;em&gt;China&lt;/em&gt; are clearly more favorable towards Clinton than Trump. One exception is the Russian media which reports 19% more positively about Trump than its American counterpart. I don’t want to get political, but I think the results for some countries aren’t much of a surprise.&lt;/p&gt;

&lt;h4 id=&quot;technical-background&quot;&gt;Technical Background&lt;/h4&gt;

&lt;p&gt;The process of doing this analysis is fairly straightforward and does not require anything except a browser and a Google account. First, I used the &lt;a href=&quot;http://gdeltproject.org/&quot;&gt;GDELT database&lt;/a&gt;, publicly available on &lt;a href=&quot;https://bigquery.cloud.google.com/table/gdelt-bq:gdeltv2.gkg&quot;&gt;Google BigQuery&lt;/a&gt;, to extract the raw data needed to create both maps. I wrote the following SQL query to do this:&lt;/p&gt;

&lt;pre class=&quot;sql&quot;&gt;&lt;code&gt;SELECT	a.country
	,AVG(CASE WHEN a.trump = 1 
		THEN a.tone ELSE NULL END) trump_tone
	,AVG(CASE WHEN a.hillary = 1
		THEN a.tone ELSE NULL END) hillary_tone
FROM (SELECT 
  cc.CountryHumanName country
  ,CASE WHEN 
  	LOWER(gkg.AllNames) LIKE &#39;%donald%trump%donald%trump%&#39;
  	THEN 1 ELSE 0 END trump
  ,CASE WHEN
  	LOWER(gkg.AllNames) LIKE &#39;%hillary%clinton%hillary%clinton%&#39;
  	THEN 1 ELSE 0 END hillary
  ,FIRST(SPLIT(gkg.V2Tone, &#39;,&#39;)) tone
FROM [gdelt-bq:gdeltv2.gkg] gkg
INNER JOIN [gdelt-bq:gdeltv2.domainsbycountry_alllangs_april2015] cc
  ON cc.Domain = gkg.SourceCommonName
WHERE (
  	LOWER(gkg.AllNames) LIKE &#39;%donald%trump%donald%trump%&#39;
  	OR LOWER(gkg.AllNames) LIKE &#39;%hillary%clinton%hillary%clinton%&#39;
  ) AND gkg.DATE &amp;gt;= 20150801000000
) a
GROUP BY a.country
HAVING	SUM(a.trump) &amp;gt;= 100
	AND SUM(a.hillary) &amp;gt;= 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the second step, I exported the results of the query as a CSV file and uploaded it to &lt;a href=&quot;https://cartodb.com/&quot;&gt;CartoDB&lt;/a&gt;, a free web service where you can create maps based on location-based data. From there on you can follow their documentation and have your maps ready in no time.&lt;/p&gt;

&lt;p&gt;From my experience, &lt;a href=&quot;https://cartodb.com/&quot;&gt;CartoDB&lt;/a&gt; is a great tool if you want to create interactive and highly customizable maps. If you only need a basic set of features, you ought to try out &lt;a href=&quot;https://www.google.com/sheets/about/&quot;&gt;Google Sheets&lt;/a&gt;. &lt;a href=&quot;http://www.tableau.com/&quot;&gt;Tableau&lt;/a&gt; is another good alternative that I frequently use, which is also available in the free version &lt;a href=&quot;https://public.tableau.com/s/&quot;&gt;Tableau Public&lt;/a&gt;. I didn’t use Tableau in this project because CartoDB offers much better embedding options for blogs or websites.&lt;/p&gt;

&lt;p&gt;If you have any questions about this blog post, feel free to contact me by &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;email&lt;/a&gt; or write me a PM on Reddit.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;&lt;small&gt;Photos by &lt;a href=&quot;https://www.flickr.com/photos/gageskidmore/&quot;&gt;Gage Skidmore&lt;/a&gt; is licensed under CC BY-SA 2.0&lt;/small&gt;&lt;/p&gt;

          </description>
        <pubDate>Wed, 27 Apr 2016 19:14:00 +0200</pubDate>
        <link>https://www.curiousgnu.com/how-the-world-sees-clinton-trump</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/how-the-world-sees-clinton-trump</guid>
      </item>
      
    
      
      <item>
        <title>Redditors who commented in /r/X also commented in /r/Y</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/rd-comments/cover.png&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;This blog article is about &lt;a href=&quot;http://www.reddit.com&quot;&gt;reddit.com&lt;/a&gt;, a website where people can post links to interesting websites and discuss a wide variety of different topics. According to &lt;a href=&quot;http://www.alexa.com/topsites/countries/US&quot;&gt;Alexa&lt;/a&gt;, a company who analyzes web traffic, Reddit is the ninth most popular site in the United States. Reddit has thousands of different subcategories, called subreddits, which are usually moderated by volunteers. There are subreddits for nearly every topic you can imagine; for example, on &lt;a href=&quot;http://www.reddit.com/r/movies&quot;&gt;/r/movies&lt;/a&gt; people can discuss the latest blockbuster whereas the users over at &lt;a href=&quot;http://www.reddit.com/r/sloths&quot;&gt;/r/sloths&lt;/a&gt; are passionately committed to collecting cute pictures of sloths.&lt;/p&gt;

&lt;p&gt;But Reddit can also be fascinating to people who are interested in data research because the user generated data is easily accessible via the official API and through &lt;a href=&quot;https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2016_01&quot;&gt;Google BigQuery&lt;/a&gt; where you can find an SQL database which you can use for little to no cost. For this article, I decided to start with something simple. My goal is to find out how the 50 most popular subreddits are related to each other. The idea behind it is that users usually write comments in subreddits which are close to their personal interests, meaning that a user who is active in the &lt;a href=&quot;http://www.reddit.com/r/StarWars&quot;&gt;/r/StarWars&lt;/a&gt; subreddit is probably also active in the &lt;a href=&quot;http://www.reddit.com/r/firefly&quot;&gt;/r/firefly&lt;/a&gt; subreddit because both categories fit his or her interest in science fiction.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/rd-comments/rd_comments_net_hd.png&quot;&gt;&lt;img src=&quot;/assets/images/rd-comments/rd_comments_net_hd.png&quot; alt=&quot;Subreddit Network&quot; style=&quot;max-height:640px; max-width:640px&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Based on this assumption, my approach was to look at all 1.2 million unique users who posted a comment in at least one of the top 50 subreddits during January 2016. To calculate the strength of the relationship between the subreddits, I used multiple logistic regression models which for example can tell us how much the probability of a Redditor posting a comment in &lt;a href=&quot;http://www.reddit.com/r/StarWars&quot;&gt;/r/StarWars&lt;/a&gt; increases if he or she also posted a comment in &lt;a href=&quot;http://www.reddit.com/r/firefly&quot;&gt;/r/firefly&lt;/a&gt;. The bigger this number is, the more closely related those subreddits are to each other. The network graph above is a visualization of these results. A bigger dot stands for a larger number of connections to its neighbors.&lt;/p&gt;

&lt;p&gt;Looking at the graph, we can identify four major groups of subreddits:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;News &amp;amp; Science&lt;/strong&gt;: &lt;a href=&quot;http://www.reddit.com/r/worldnews&quot;&gt;/r/worldnews&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/science&quot;&gt;/r/science&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/space&quot;&gt;/r/space&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/futurology&quot;&gt;/r/futurology&lt;/a&gt;, …&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Entertainment&lt;/strong&gt;: &lt;a href=&quot;http://www.reddit.com/r/movies&quot;&gt;/r/movies&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/television&quot;&gt;/r/television&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/music&quot;&gt;/r/music&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/books&quot;&gt;/r/books&lt;/a&gt;, …&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Visual Content&lt;/strong&gt;: &lt;a href=&quot;http://www.reddit.com/r/funny&quot;&gt;/r/funny&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/pics&quot;&gt;/r/pics&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/aww&quot;&gt;/r/aww&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/creepy&quot;&gt;/r/creepy&lt;/a&gt;, …&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Textual Content&lt;/strong&gt;: &lt;a href=&quot;http://www.reddit.com/r/showerthought&quot;&gt;/r/showerthought&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/askreddit&quot;&gt;/r/askreddit&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/tifu&quot;&gt;/r/tifu&lt;/a&gt;, &lt;a href=&quot;http://www.reddit.com/r/lifeprotips&quot;&gt;/r/lifeprotips&lt;/a&gt;, …&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The subreddit &lt;a href=&quot;http://www.reddit.com/r/todayilearned&quot;&gt;/r/todayilearned&lt;/a&gt; doesn’t belong to any particular group because it’s somewhat popular among all users. This analysis doesn’t go into great detail, but I think it’s nevertheless interesting to see that the groups of subreddit seem to make sense and can be interpreted. For example, it doesn’t sound wrong that users who enjoy commenting on topics about space are also interested in science.&lt;/p&gt;

&lt;p&gt;Additionally, I also made a table from the same data. The software programs I used to create both graphs are &lt;a href=&quot;https://gephi.org/&quot;&gt;Gephi&lt;/a&gt; and &lt;a href=&quot;https://public.tableau.com/s/&quot;&gt;Tableau&lt;/a&gt; respectively. A blue square stands for a positive correlation coefficient whereas a red square represents the opposite.  You can open the full table by clicking on the graph below:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://public.tableau.com/profile/curious.gnu#!/vizhome/RedditComments-LogitRegressioncuriousgnu_com/Sheet1&quot;&gt;&lt;img src=&quot;/assets/images/rd-comments/rd_table.png&quot; alt=&quot;Subreddit Table&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Admittedly, these aren’t exactly groundbreaking results, but it was real fun to try out some statistical methods on this huge amount of data. I’m currently testing how I can use this data source for an article about text analysis.&lt;/p&gt;

          </description>
        <pubDate>Wed, 20 Apr 2016 14:14:00 +0200</pubDate>
        <link>https://www.curiousgnu.com/reddit-comments</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/reddit-comments</guid>
      </item>
      
    
      
      <item>
        <title>Penny Auctions - How to sell a $180 tablet for $7,264</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/penny-auctions/cover.png&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;Unless you use an ad blocker, you probably notice ads for penny auction sites from time to time. They usually advertise with sketchy messages like &lt;em&gt;“iPhone sold for $14.21.”&lt;/em&gt; They can sell iPhones such low prices because of their unusual auction system where each bid increases the auction price by only one cent. This works because unlike eBay each bid costs money (e.g., $0.40) no matter if you end up winning the auction or not. You have to be the highest bidder when the clock runs out to win the auction. The problem is that each bid adds ten seconds to the countdown which gives other bidders the time to counter your bid.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/penny-auctions/mockup_paad.jpg&quot; alt=&quot;Penny Auction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Penny auction sites are not something new and have been criticized a lot, which makes you wonder how they are still in business. Even though there are many news articles about online penny auctions, I did not find any numbers or statistics which would support the criticism. So I started to collect information on my own, to get a better understanding how these auctions work in reality. What I found exceeded all my expectations.&lt;/p&gt;

&lt;p&gt;On &lt;a href=&quot;https://en.wikipedia.org/wiki/Beezid&quot;&gt;beezid.com&lt;/a&gt;, one of the bigger penny auction sites, a single $180 tablet generated 18,160 bids, worth of over &lt;strong&gt;$7,200&lt;/strong&gt;, from 56 users. Shockingly nearly half of those bids came from just one person who lost approximately lost &lt;strong&gt;$3,500&lt;/strong&gt; in just two hours (see graph below). The winner of this auction only spent 80 cents on his or her two bids. In the second half of this blog post, I will outline the data collection process. To protect the users’ privacy, I replaced the real usernames with chemical elements.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/penny-auctions/money_spent_hd.png&quot;&gt;&lt;img src=&quot;/assets/images/penny-auctions/money_spent.png&quot; alt=&quot;Money Spent on Bids&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;why-are-penny-auctions-even-popular&quot;&gt;Why are penny auctions even popular?&lt;/h4&gt;
&lt;p&gt;The big ‘achievement’ of penny auction sites is that they successfully turned an unappealing type of an all-pay auction into an online game which makes people believe that they could make a profit.  The way beezid.com does this is actually quite clever. First, they give their users many different options how they can make a bid. Users can, for example, use automatic bidding bots, take advantage of price limits above which the auction price will not rise, and purchase a wide variety of other supposedly useful add-ons. Second, they try to hide all information that could reveal how many people are participating in the auction and how much they already spent in total.&lt;/p&gt;

&lt;p&gt;One way they do this is by their default 10% price limit, which means the auction price will never rise above 10% of the retail price, no matter how many people bid on it. Another strange rule is that bids do not always increase the auction price by one cent but can also lower it by some amount. These modifications of the system make the auction price more or less meaningless because you can no longer assume that, for example, a price of $1.10 is a result of 110 unique bids. Many articles about penny auctions (e.g., &lt;a href=&quot;http://www.consumerreports.org/cro/2011/12/with-penny-auctions-you-can-spend-a-bundle-but-still-leave-empty-handed/index.htm&quot;&gt;consumerreports.org&lt;/a&gt;) do not account for these rules and falsely assume a fixed price increase. The graph below shows the development of the auction price of the $180 tablet over time. If we assume, that each bid would have increased the price by always one cent, the final auction price would have been &lt;strong&gt;$181.60&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/penny-auctions/auction_price_hd.png&quot;&gt;&lt;img src=&quot;/assets/images/penny-auctions/auction_price.png&quot; alt=&quot;Auction Price&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This lack of transparency combined with the many different bidding options creates a system which gives users the illusion that they could make a profit with the right strategies.&lt;/p&gt;

&lt;h4 id=&quot;how-to-monitor-penny-auctions&quot;&gt;How to monitor penny auctions?&lt;/h4&gt;
&lt;p&gt;Despite the site’s efforts to keep their users in the dark, it is possible to archive the complete bidding history of an auction, if you monitor it from its start. To collect the data for this blog post, I wrote a simple &lt;a href=&quot;https://gist.github.com/CuriousGnu/cf558e8ca8cbbd491619a9e1940682c3&quot;&gt;script&lt;/a&gt; in Python which automatically saves the all bidding information of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Beezid&quot;&gt;beezid.com&lt;/a&gt; auction. If you want to try it out yourself, you only have to change the auction id and update the request header. You can easily get this information with the Chrome DevTools. In case, you have a slow or unstable internet connection I highly recommend running the script on a server (e.g., &lt;a href=&quot;https://www.digitalocean.com/pricing/&quot;&gt;Digital Ocean&lt;/a&gt;). It should be noted that the script behaves like a regular browser and does not bypass any server-side security or content protection mechanisms.&lt;/p&gt;

&lt;h4 id=&quot;arent-penny-auctions-unregulated-gambling&quot;&gt;Aren’t penny auctions unregulated gambling?&lt;/h4&gt;
&lt;p&gt;One of the most important parts of the business models of penny auction sites is that what they are doing is not considered gambling. I am not a lawyer, so I will not attempt to question the legality of their operations. Their central argument is that penny auctions require skill and are therefore exempt from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Unlawful_Internet_Gambling_Enforcement_Act_of_2006&quot;&gt;Unlawful Internet Gambling Enforcement Act&lt;/a&gt;, which is the same legal loophole daily fantasy sports sites like &lt;a href=&quot;https://en.wikipedia.org/wiki/FanDuel&quot;&gt;FanDuel&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/DraftKings&quot;&gt;DraftKings&lt;/a&gt; use. Personally, I cannot see how any skill could increase your chances of winning an auction. The data clearly shows that you are bidding against an unknown number of users who sometimes act extremely irrational. Even if you had a comprehensive database of previous auctions, you probably would not be able to predict how far a specific user will go. If someone has already spent over $3,000 on a $180 tablet, what could stop them aside from his or her credit card limit? It would be really interesting to hear an official explanation of how skill is even a factor in this game.&lt;/p&gt;

&lt;p&gt;If you have any questions about this blog post, feel free to contact me by &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;email&lt;/a&gt;.&lt;/p&gt;

          </description>
        <pubDate>Mon, 04 Apr 2016 20:00:00 +0200</pubDate>
        <link>https://www.curiousgnu.com/penny-auctions</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/penny-auctions</guid>
      </item>
      
    
      
      <item>
        <title>How positive are your tweets?</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/tweetanalyzer/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;In this blog post, I would like to present you &lt;a href=&quot;https://tweetanalyzer.net&quot;&gt;tweetanalyzer.net&lt;/a&gt;, a small project of mine, where you can do a sentiment analysis of your tweets or the ones of any other Twitter user. The goal was to create a fun website which uses text analysis to determine how positive someone’s tweets are.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://tweetanalyzer.net&quot;&gt;&lt;img src=&quot;/assets/images/tweetanalyzer/screen.png&quot; alt=&quot;TweetAnalyzer.net&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After you signed in with your Twitter account, you can type in any username and the website will automatically download the latest 200-300 tweets and calculate a positivity and vulgarity score for each of them. The method used to do this is very simple. The backend uses the AFINN word list from &lt;a href=&quot;http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010&quot;&gt;Finn Årup Nielsen&lt;/a&gt;, which contains nearly 2,500 words rated for their positivity from -5 (negative) to +5 (positive). After removing stop words, hashtags, and URLs the script looks up every word in this list and calculates the sum for each tweet. If this sum is over zero, the tweet will be marked as positive if it’s under zero as negative. The overall positivity score for a user is then calculated as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tweetanalyzer/positivity_score.gif&quot; alt=&quot;PSF&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Admintingly this approach has its weakness and is not as sophisticated as many other methods classification technique out there. For example, it can not understand the context of a tweet and how a specific word (e.g., &lt;em&gt;sick&lt;/em&gt;) is used. I experimented more complex Python libraries for text analysis, but unfortunately they did not run well on the Google App Engine platform if you plan to analyse thousands of tweets but only have a limited budget. I am sure that it can be done without a problem but since I never used it before and did not want to spend too much time on it, I decided to use the simple solution which I believe is still sufficient for such a task.&lt;/p&gt;

&lt;p&gt;To determine how vulgar a tweet is, the software uses a similar word list based method. If a tweet contains a word which is in this list, it will be marked as &lt;em&gt;vulgar&lt;/em&gt;. The lack of content-awareness will, of course, lead to some mistakes. For example, news articles about &lt;em&gt;sex trafficking&lt;/em&gt; or &lt;em&gt;rape&lt;/em&gt; will be falsely classified as &lt;em&gt;vulgar&lt;/em&gt;. So please do not take the results too seriously and rather tweet something positive about it! Feel free to &lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#099;&amp;#111;&amp;#110;&amp;#116;&amp;#097;&amp;#099;&amp;#116;&amp;#064;&amp;#099;&amp;#117;&amp;#114;&amp;#105;&amp;#111;&amp;#117;&amp;#115;&amp;#103;&amp;#110;&amp;#117;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;contact me&lt;/a&gt; if you suggestions or questions.&lt;/p&gt;

&lt;p&gt;You can find the project here: &lt;a href=&quot;https://tweetanalyzer.net&quot;&gt;https://tweetanalyzer.net&lt;/a&gt;&lt;/p&gt;

          </description>
        <pubDate>Wed, 16 Mar 2016 22:42:00 +0100</pubDate>
        <link>https://www.curiousgnu.com/tweetanalyzer</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/tweetanalyzer</guid>
      </item>
      
    
      
      <item>
        <title>What are the most common words in TV shows?</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/tv-wordcloud/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;Nowadays, there are some many great shows on television or available for streaming. After binge-watching the first season of Jessica Jones, I knew that I had to post something about this topic. I have to admit that I never spend much time on text analysis, so I decided to start with the basics. The first thing I did was to create simple word clouds based on the subtitles of &lt;em&gt;Jessica Jones&lt;/em&gt;, &lt;em&gt;The Blacklist&lt;/em&gt;, and &lt;em&gt;TBBT&lt;/em&gt;. They are an easy way to visualize how frequently certain words are used in a text or TV show.&lt;/p&gt;

&lt;p&gt;Besides this basic type of word cloud, I also created a slightly different version of it. For this, I did not just count the words but also looked how close they are to a character’s name. This means that the more frequent a word is used in combination with a character’s name, the bigger it is in the word cloud.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/tv-wordcloud/wordcloud_jessicajones_hd.png&quot;&gt;&lt;img src=&quot;/assets/images/tv-wordcloud/wordcloud_jessicajones.png&quot; alt=&quot;Jessica Jones Wordcloud&quot; title=&quot;Jessica Jones Wordcloud&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/tv-wordcloud/wordcloud_blacklist_hd.png&quot;&gt;&lt;img src=&quot;/assets/images/tv-wordcloud/wordcloud_blacklist.png&quot; alt=&quot;The Blacklist Wordcloud&quot; title=&quot;The Blacklist Wordcloud&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/tv-wordcloud/wordcloud_tbbt_hd.png&quot;&gt;&lt;img src=&quot;/assets/images/tv-wordcloud/wordcloud_tbbt.png&quot; alt=&quot;TBBT Wordcloud&quot; title=&quot;TBBT Wordcloud&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I created all those word clouds with the R package &lt;a href=&quot;https://cran.r-project.org/web/packages/wordcloud/index.html&quot;&gt;&lt;em&gt;wordcloud&lt;/em&gt;&lt;/a&gt; which makes this job so easy. Besides that, the &lt;em&gt;findAssocs()&lt;/em&gt; function from the &lt;a href=&quot;https://cran.r-project.org/web/packages/tm/index.html&quot;&gt;&lt;em&gt;tm&lt;/em&gt;&lt;/a&gt; package was used calculate the often a word is used close to a character’s name. For the second type of word clouds, I used the correlation coefficients from this function as a weighting factor.&lt;/p&gt;

&lt;p&gt;Before doing that, I replaced all the different versions of a name (e.g., Lizzy, Liz, Keen) with the name you can see in the center of the word clouds. In case you are wondering why there are not always complete words in the clouds, it is because of a process called &lt;em&gt;stemming&lt;/em&gt; which reduces words to their word stem. The idea behind it is to group the different forms of a word together instead of treating every single form as an own term.&lt;/p&gt;

&lt;p&gt;I hope you like this post. Please feel free to contact me if you have any questions or suggestions.&lt;/p&gt;

          </description>
        <pubDate>Sat, 27 Feb 2016 13:42:00 +0100</pubDate>
        <link>https://www.curiousgnu.com/tv-show-word-clouds</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/tv-show-word-clouds</guid>
      </item>
      
    
      
      <item>
        <title>Who sells LSD on the Darktnet?</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/darknet-lsd/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;I recently got my hands on Diana S. Dolliver’s &lt;a href=&quot;http://dx.doi.org/10.1016/j.drugpo.2015.01.008&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt; about drug dealing on the Tor network, a hidden, uncensored network which can only be accessed by &lt;a href=&quot;https://www.torproject.org/&quot; target=&quot;_blank&quot;&gt;special software&lt;/a&gt;. I got interested this topic because it allows us to analyse transactions which used to be hidden and nearly impossible to observe.&lt;/p&gt;

&lt;p&gt;Besides the many legitimate use cases of the Tor network, it also provides together with Bitcoin the technology most &lt;em&gt;cryptomarkets&lt;/em&gt; use. Tor allows the side administrators to host their sites anonymously without the fear of getting arrested immediately by the FBI. Cryptomarkets function similar to eBay, in the sense that they do not sell drugs directly but provide a platform where other vendors can sell their products for a small fee. The screenshot below shows how such a site looks from a buyers perspective. I chose to take a close look at the &lt;em&gt;AlphaBay Market&lt;/em&gt;, one of the biggest platforms in December 2015.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/darknet-lsd/screenshot.png&quot;&gt;&lt;img src=&quot;/assets/images/darknet-lsd/screenshot.png&quot; alt=&quot;AlphaBay Market Screenshot&quot; title=&quot;AlphaBay Market Screenshot&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h5 id=&quot;how-to-analyse-the-alphabay-market&quot;&gt;How to analyse the AlphaBay Market?&lt;/h5&gt;

&lt;p&gt;If you access one of the larger cryptomarkets, you will probably see thousands of different offerings ranging from prescription drugs to illegal weapons. The fact that anonymous users claim to sell those products, of course, does not say anything about the actual demand and volume of the sales. This is the point where it gets interesting because the &lt;em&gt;AlphaBay Market&lt;/em&gt; has a revealing feedback system which lists not only lists all products sold but also their prices, and purchase dates.&lt;/p&gt;

&lt;p&gt;Assuming a constant feedback ratio, we can use this data to observe the early development of the market from only one snapshot. To automatically download the customer feedbacks I wrote a scraper in Python, which provided me with the dataset for the further analysis.&lt;/p&gt;

&lt;h5 id=&quot;how-much-lsd-is-sold&quot;&gt;How much LSD is sold?&lt;/h5&gt;

&lt;p&gt;Unfortunately, the scraping process of a website hidden in the Tor network is a quite slow. Therefore, I decided only to download information related to LSD sales. The diagram below shows last year’s LSD sales for the weeks 13 to 50:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/darknet-lsd/plot_hd.png&quot;&gt;&lt;img src=&quot;/assets/images/darknet-lsd/plot.png&quot; alt=&quot;LSD Sales&quot; title=&quot;LSD Sales&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;During November 2015, approximately &lt;strong&gt;$215,000&lt;/strong&gt; worth of LSD was sold on the AlphaBay Market. I do not have a definitive explanation for the sales increase following week 44, but my guess is that problems with competing cryptomarkets drove new sellers to this platform and their existing customers followed. This theory is supported by the fact that already established vendors like &lt;em&gt;Lyseric025&lt;/em&gt; were not able to significantly increase their revenue during this time.&lt;/p&gt;

&lt;h5 id=&quot;who-are-the-dealers&quot;&gt;Who are the dealers?&lt;/h5&gt;

&lt;p&gt;To better get a better understanding of who those LSD vendors are, we need to look at their other sales also:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/darknet-lsd/table_hd.png&quot;&gt;&lt;img src=&quot;/assets/images/darknet-lsd/table.png&quot; alt=&quot;LSD Vendors&quot; title=&quot;LSD Vendors&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We can see in the table above that the leading LSD seller on the AlphaBay Market was &lt;em&gt;Lyseric025&lt;/em&gt;, who had a market share of 29% in November 2015 and generated a revenue of $65,438. Only 3% of it came from B2B sales (sale over $200) which suggest a strong focus on the end customer.&lt;/p&gt;

&lt;p&gt;Interestingly most sellers on this list made the majority of their revenue with LSD. One of the reasons for this could be that the LSD is supposedly produced by only a &lt;a href=&quot;http://www.justice.gov/archive/ndic/pubs25/25921/25921p.pdf&quot; target=&quot;_blank&quot;&gt;few producers&lt;/a&gt; which wholesalers are probably only interested in selling in large volumes. This would mean that a specialized vendor could increase its profit by using economies of scale. I wonder if there is a similar specialization in other more decentralized drug markets.&lt;/p&gt;

          </description>
        <pubDate>Sat, 16 Jan 2016 12:10:00 +0100</pubDate>
        <link>https://www.curiousgnu.com/cryptomarkets-lsd-sales</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/cryptomarkets-lsd-sales</guid>
      </item>
      
    
      
      <item>
        <title>What&#39;s the age difference in movies?</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/age-gap/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;In response to my &lt;a href=&quot;/imdb-age-distribution&quot;&gt;last post&lt;/a&gt; about the distribution of ages of actresses and actors, a Reddit user suggested investigating further the age difference between female and male movie stars. In my first simple analysis, I only compared the average ages of both groups in the overall sample. Now, the next step is to check on a movie basis if there is an age difference between the leading actress and her co-star.&lt;/p&gt;

&lt;h5 id=&quot;methods&quot;&gt;Methods&lt;/h5&gt;
&lt;p&gt;First I needed to identify the two leads of each film (2000-2015, with 10,000+ votes) to calculate the age difference. Unfortunately, there is no direct way to get this information from the IMDb dataset. I, therefore, used a quick-and-dirty approve and chose the first actress and actor in the IMDb cast credits as leads. If I did not find both an actress and an actor in the first three entries (in credits order), I removed the whole movie from the sample. With this method, I was able to get the leading actresses and actors of 1,201 movies. Note that they are not necessarily movie couples. I am aware that a manually selected sample would have been better but considering the simplicity of my approach; I think that the results are acceptable for this analysis. Feel free to check it yourself &lt;a href=&quot;/assets/data/age_gap_2015.csv&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h5 id=&quot;results&quot;&gt;Results&lt;/h5&gt;
&lt;p&gt;When we look at the box-plot of the age differences, we can see that on average the leading actor is &lt;strong&gt;five years older&lt;/strong&gt; than his female co-star. It also tells us that in approximately 75% of all movies in our sample have an older male lead. Based on this data, the answer to the question from the title is &lt;em&gt;“Yes”&lt;/em&gt; even though the age gap might be not as big as expected.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/age-gap/plot_2.png&quot;&gt;&lt;img src=&quot;/assets/images/age-gap/plot_2.png&quot; alt=&quot;Age Gap Boxplot&quot; title=&quot;Age Gap Boxplot&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Finally, I also created a density plot which summarizes all data points in a nice looking, colorful graph. The area below the gray dashed line stands for all the movies in which the leading actor is older :&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/age-gap/plot_1_hd.png&quot;&gt;&lt;img src=&quot;/assets/images/age-gap/plot_1.png&quot; alt=&quot;Age Gap Plot&quot; title=&quot;Age Gap Plot&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

          </description>
        <pubDate>Wed, 04 Nov 2015 13:21:00 +0100</pubDate>
        <link>https://www.curiousgnu.com/imdb-age-gap</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/imdb-age-gap</guid>
      </item>
      
    
      
      <item>
        <title>Actresses Are on Average 7 Years Younger</title>
          <description>
            &lt;img class=&quot;webfeedsFeaturedVisual&quot; src=&quot;https://www.curiousgnu.com/assets/images/age-dist/cover.jpg&quot;  alt=&quot;Cover Image&quot; /&gt;
            &lt;p&gt;The IMDb dataset is a great source for everyone loves movies and numbers. After I had figured out how to &lt;a href=&quot;http://imdbpy.sourceforge.net/support.html&quot; target=&quot;_blank&quot;&gt;import the data&lt;/a&gt; into a local SQL database, the first thing I did was to look at the age of the actresses and actors during filming.&lt;/p&gt;

&lt;p&gt;It turns out that the average actress is, with a median age of 32, &lt;strong&gt;seven years younger&lt;/strong&gt; than her male counterpart who is on average 39 years old. The diagram below shows that the distribution of ages of actresses is more skewed to the right (γ1=.912) than those of ages of actors (γ1=.483). This suggests that there is a relatively higher demand for actresses under the age of 35. Interestingly we do not see such an apparent age preference in the casting of actors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/images/age-dist/plot_hd.png&quot;&gt;&lt;img src=&quot;/assets/images/age-dist/plot.png&quot; alt=&quot;Age Distribution Plot&quot; title=&quot;Age Distribution Plot&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The sample consists of all roles played by actresses (n=21,551) and actors (n=50,165) in U.S. movie released between 2000 and 2015, with more than 10,000 IMDb-Votes. By the way, in the United States, the median age of females and males is 39.2 years and 36.5 years respectively (&lt;a href=&quot;https://www.cia.gov/library/publications/the-world-factbook/geos/us.html&quot;&gt;The World Factbook, 2015&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;small&gt;Photo: “California Movie Theater Berkeley” by &lt;a href=&quot;https://flic.kr/p/aM4GwX&quot;&gt;Russell Mondy&lt;/a&gt; is licensed under CC BY-NC 2.0&lt;/small&gt;&lt;/p&gt;

          </description>
        <pubDate>Mon, 02 Nov 2015 21:10:00 +0100</pubDate>
        <link>https://www.curiousgnu.com/imdb-age-distribution</link>
        <guid isPermaLink="true">https://www.curiousgnu.com/imdb-age-distribution</guid>
      </item>
      
    
  </channel>
</rss>